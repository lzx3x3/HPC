{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is meant to be run on **four nodes** with 28 cores.\n",
    "\n",
    "One thing we are trying to do is gauge the performance of a good MPI implementation and compare it to a good OpenMP implementation.  In order to do this, we will have to load some slightly different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module unload cse6230\n",
    "module load cse6230/gcc-omp-gnu\n",
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([mvapich](http://mvapich.cse.ohio-state.edu/) is a fork of the [mpich](https://www.mpich.org/) MPI implementation with some modifications for performance on various HPC hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring MPI primitives\n",
    "\n",
    "**Task 1 (3 pts)** The file `benchmarks.c` includes some basic benchmarks for measuring the performance of various MPI point-to-point and collective operations.  Right now, it is incomplete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"TODO\" benchmarks.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refering to a good [MPI Tutorial](https://computing.llnl.gov/tutorials/mpi/) or\n",
    "[lecture notes](http://vuduc.org/cse6230/slides/cse6230-fa14--06-mpi.pdf) as needed,\n",
    "fill in the missing MPI routines.\n",
    "\n",
    "Once you have done that, run the following script to generate a graph of benchmark bandwidths of MPI routines.  Note that these values are only for MPI messages within a node: values may be different when we start using multiple nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runbenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display < benchmarks.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Right now the graph is showing no values because the \"timing\" values are negative until you complete the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 pts)** We've talked in class about a simplified model of the cost of an MPI message: $\\lambda + g b$, where $\\lambda$ is the latency and $g$ is the inverse bandwidth.\n",
    "Using your graph for Send/Recv bandwidths for different message sizes (which was simply calculated from dividing the message size by the message time), estimate $\\lambda$ (units secs) and $g$ (units secs/byte) for this MPI implementation on these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (3 pts)** The point-to-point sends and receives in MPI are _symmetric_: there must be a receive for each send.  What if we start with a list of messages that is _asymmetric_: we know who to send to, but not who to receive from.\n",
    "\n",
    "Suppose rank $i$ has $N_i$ messages bound for receivers $r_{i,j}$ for $1 \\leq j \\leq N_i$.  Let $S_{i,j}$ be the size of the message from $i$ to $r_{i,j}$, and let $S_i = \\sum_j S_{i,j}$ be the total outgoing message volume from rank $i$.\n",
    "\n",
    "Process $r_{i,j}$ does not know it is going to receive a message from $i$.\n",
    "\n",
    "Give pseudocode below for two algorithms, using MPI sends, receives, and collectives that we have talked about, to send all of the messages.\n",
    "\n",
    "In the first one, assume that there is a large volume of communication: that $S_i \\in O(P)$ for each $i$.\n",
    "\n",
    "In the second one, assume that there is a small volume of communication, and a small number of communicators:\n",
    "$S_i \\in O(1)$, and each rank needs to send or receive a message from $O(1)$ other processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
