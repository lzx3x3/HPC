{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Matrix-Vector Product, 2D Decomposition\n",
    "\n",
    "This assignment will be graded on four nodes with 28 cores, but for development you can run anywhere.\n",
    "\n",
    "**(Coding, 6 pt):** In this assignment you will complete the dense matrix vector product code where the matrix is decomposed by a 2D decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module unload cse6230\n",
    "module load cse6230/gcc-omp-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o dense_matrix_vector_product\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dense_matrix_vector_product.o dense_matrix_vector_product.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_args.o dmv_args.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv.o dmv.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_global_size.o dmv_global_size.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_layout.o dmv_layout.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_2d.o dmv_matvec_2d.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec.o dmv_matvec.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_col.o dmv_matvec_col.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_row.o dmv_matvec_row.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_offset.o dmv_offset.c\n",
      "mpicc -o dense_matrix_vector_product dense_matrix_vector_product.o dmv_args.o dmv.o dmv_global_size.o dmv_layout.o dmv_matvec_2d.o dmv_matvec.o dmv_matvec_col.o dmv_matvec_row.o dmv_offset.o -lm -lrt\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "make dense_matrix_vector_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dense_matrix_vector_product` program computes distributed dense matrix vector products by a few different distributions and algorithms that you can choose from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: ./dense_matrix_vector_product [-s scale] [-e seed] [-v verbosity] [-d debug]\n",
      "          [-g global_size_strategy] [-l layout_strategy]\n",
      "          [-p matrix_partition_strategy] [-m matvec_strategy]\n",
      "\n",
      "global_size_strategy:tree_subcomm, tree_recurse, reduce_bcast, allreduce\n",
      "layout_strategy: gather, allgather, scan\n",
      "matrix_partition_strategy: rows, cols, 2d\n",
      "matvec_strategy: ssend, issend, allgathervRight vector norm: 18.6581\n",
      "Matrix Frobenius norm: 596.735\n",
      "[0] Average time per matvec: 0.000976378\n",
      "Left vector norm: 318.194\n"
     ]
    }
   ],
   "source": [
    "mpirun -f ${PBS_NODEFILE} -np 1 ./dense_matrix_vector_product -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector and matrix entries are computed from a random hash of their global indices, meaning that they should be the same regardless of the layout or the number of processes.  A quick check on this is to compute the norms of the objects involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right vector norm: 18.6581\n",
      "Matrix Frobenius norm: 596.735\n",
      "[0] Average time per matvec: 0.000951043\n",
      "Left vector norm: 318.194\n",
      "Right vector norm: 18.6581\n",
      "Matrix Frobenius norm: 596.735\n",
      "[0] Average time per matvec: 0.000147663\n",
      "Left vector norm: 318.194\n",
      "Right vector norm: 18.6581\n",
      "Matrix Frobenius norm: 596.735\n",
      "[0] Average time per matvec: 0.000125209\n",
      "Left vector norm: 318.194\n"
     ]
    }
   ],
   "source": [
    "mpirun -f ${PBS_NODEFILE} -np 1 ./dense_matrix_vector_product -p rows\n",
    "mpirun -f ${PBS_NODEFILE} -np 8 ./dense_matrix_vector_product -p rows\n",
    "mpirun -f ${PBS_NODEFILE} -np 8 ./dense_matrix_vector_product -p cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D partitioned version is not written yet: as such, the matrix and left vector norms are wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right vector norm: 18.6581\n",
      "Matrix Frobenius norm: 596.735\n",
      "[0] Average time per matvec: 0.000191717\n",
      "Left vector norm: 318.194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mpirun -f ${PBS_NODEFILE} -np 8 ./dense_matrix_vector_product -p 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the code, you must write the following routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m/* Given arguments (which include a communicator, args->comm),\u001b[39;49;00m\n",
      "\u001b[37m * offsets for each rank in the left and right vectors compatible with a matrix (lOffsets and rOffsets),\u001b[39;49;00m\n",
      "\u001b[37m * compute which entries in the matrix this MPI rank will own, given by the row ranges [mStart, mEnd) and column ranges [nStart, nEnd) */\u001b[39;49;00m\n",
      "\u001b[36mint\u001b[39;49;00m \u001b[32mMatrixGetLocalRange2d\u001b[39;49;00m(Args args, \u001b[34mconst\u001b[39;49;00m \u001b[36mint\u001b[39;49;00m *lOffsets, \u001b[34mconst\u001b[39;49;00m \u001b[36mint\u001b[39;49;00m *rOffsets, \u001b[36mint\u001b[39;49;00m *mStart_p, \u001b[36mint\u001b[39;49;00m *mEnd_p, \u001b[36mint\u001b[39;49;00m *nStart_p, \u001b[36mint\u001b[39;49;00m *nEnd_p)\n",
      "{\n",
      "  MPI_Comm comm = args->comm;\n",
      "  \u001b[36mint\u001b[39;49;00m      mStart, mEnd, nStart, nEnd;\n",
      "  \u001b[36mint\u001b[39;49;00m      size, rank;\n",
      "  \u001b[36mint\u001b[39;49;00m      err;\n",
      "  \u001b[36mint\u001b[39;49;00m num_cols, num_rows, col, row;\n",
      "\n",
      "  \u001b[37m/* initialize to bogus values */\u001b[39;49;00m\n",
      "  mStart = mEnd = nStart = nEnd = -\u001b[34m1\u001b[39;49;00m;\n",
      "  err = MPI_Comm_size(comm, &size); MPI_CHK(err);\n",
      "  err = MPI_Comm_rank(comm, &rank); MPI_CHK(err);\n",
      "  \u001b[37m/* TODO: compute mStart, mEnd, nStart, and nEnd. HINT: use DMVCommGetRankCoordinates2D() to get the\u001b[39;49;00m\n",
      "\u001b[37m   * number of block columns and rows used to partition the matrix, mBlock and nBlock.\u001b[39;49;00m\n",
      "\u001b[37m   * The block row i should contain the same rows as are in the left vector for\u001b[39;49;00m\n",
      "\u001b[37m   * ranks (i * nBlock, i * nBlock + 1, ..., (i + 1) * nBlock - 1).\u001b[39;49;00m\n",
      "\u001b[37m   * The block column j should contain the same columns as are in the right\u001b[39;49;00m\n",
      "\u001b[37m   * vector for ranks (j * mBlock, j * mBlock + 1, ..., (j + 1) * mBlock - 1).\u001b[39;49;00m\n",
      "\u001b[37m   */\u001b[39;49;00m\n",
      "\n",
      "  DMVCommGetRankCoordinates2D(comm, &num_rows, &row, &num_cols, &col);\n",
      "  \n",
      "  mStart = lOffsets[row*num_cols];\n",
      "  mEnd = lOffsets[(row+\u001b[34m1\u001b[39;49;00m)*num_cols];\n"
     ]
    }
   ],
   "source": [
    "grep -B 3 -A 23 \"MatrixGetLocalRange2d\" dmv.c | pygmentize -l C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `lOffsets` and `rOffsets` are like the `displs` argument to [MPI_Gatherv]: (https://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html) they are arrays of length `size + 1` (where `size` is the size of the MPI Communicator).\n",
    "\n",
    "To complete this routine, you should figure out how you are going to set up your 2D grid of MPI processes in the following routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m/* Given the number of processes in the MPI communicator, compute a 2D\u001b[39;49;00m\n",
      "\u001b[37m * grid size (num_rows x num_cols) for the communicator and the coordinates of\u001b[39;49;00m\n",
      "\u001b[37m * the current rank (row, col).  Make sure to use a column major ordering,\u001b[39;49;00m\n",
      "\u001b[37m * so that rank 1 gets coordates (1, 0), not (0,1) */\u001b[39;49;00m\n",
      "\u001b[36mint\u001b[39;49;00m \u001b[32mDMVCommGetRankCoordinates2D\u001b[39;49;00m(MPI_Comm comm, \u001b[36mint\u001b[39;49;00m *num_rows_p, \u001b[36mint\u001b[39;49;00m *row_p, \u001b[36mint\u001b[39;49;00m *num_cols_p, \u001b[36mint\u001b[39;49;00m *col_p)\n",
      "{\n",
      "  \u001b[36mint\u001b[39;49;00m num_cols, num_rows, col, row;\n",
      "  \u001b[36mint\u001b[39;49;00m size, rank, err;\n",
      "  \u001b[36mint\u001b[39;49;00m dims[\u001b[34m2\u001b[39;49;00m]= {\u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m};\n",
      "  \n",
      "  err = MPI_Comm_size(comm, &size); MPI_CHK(err);\n",
      "  err = MPI_Comm_rank(comm, &rank); MPI_CHK(err);\n",
      "    \n",
      "  num_cols = num_rows = col = row = -\u001b[34m1\u001b[39;49;00m;\n",
      "  \u001b[37m/* TODO: HINT, lookup MPI_Dims_create() */\u001b[39;49;00m\n",
      "  MPI_Dims_create(size , \u001b[34m2\u001b[39;49;00m, dims);\n"
     ]
    }
   ],
   "source": [
    "grep -m 1 -B 5 -A 11 \"DMVCommGetRankCoordinates2D\" dmv.c | pygmentize -l C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've computed the range of entries for a process, the program will fill those entries.  The only other thing you need to do is the actual matvec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m#\u001b[39;49;00m\u001b[36minclude\u001b[39;49;00m \u001b[37m\"dmv.h\"\u001b[39;49;00m\u001b[36m\u001b[39;49;00m\n",
      "\u001b[36m#\u001b[39;49;00m\u001b[36minclude\u001b[39;49;00m \u001b[37m\"dmv_impl.h\"\u001b[39;49;00m\u001b[36m\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mint\u001b[39;49;00m \u001b[32mDenseMatVec_2dPartition\u001b[39;49;00m(Args args, \u001b[36mint\u001b[39;49;00m mStart, \u001b[36mint\u001b[39;49;00m mEnd, \u001b[36mint\u001b[39;49;00m nStart, \u001b[36mint\u001b[39;49;00m nEnd, \u001b[34mconst\u001b[39;49;00m \u001b[36mdouble\u001b[39;49;00m *matrixEntries, \n",
      "                            \u001b[36mint\u001b[39;49;00m rStart, \u001b[36mint\u001b[39;49;00m rEnd, \u001b[34mconst\u001b[39;49;00m \u001b[36mdouble\u001b[39;49;00m *vecRightLocal, \u001b[36mint\u001b[39;49;00m lStart, \u001b[36mint\u001b[39;49;00m lEnd, \u001b[36mdouble\u001b[39;49;00m *vecLeftLocal)\n",
      "{\n",
      "  MPI_Comm comm = args->comm;\n",
      "  MPI_Comm rowComm, colComm;\n",
      "  \u001b[36mint\u001b[39;49;00m      size, rank, err, row, col, buffsize=\u001b[34m0\u001b[39;49;00m, pairDest=-\u001b[34m1\u001b[39;49;00m, pairOrigin=-\u001b[34m1\u001b[39;49;00m;\n",
      "  \u001b[36mint\u001b[39;49;00m num_cols, num_rows;\n",
      "  \u001b[36mdouble\u001b[39;49;00m *vecRight = \u001b[36mNULL\u001b[39;49;00m, *recvBuf;\n",
      "  \u001b[36mint\u001b[39;49;00m *nOffsets = \u001b[36mNULL\u001b[39;49;00m, *nLocals, *mLocals;\n",
      "  \u001b[36mint\u001b[39;49;00m rLocal = rEnd - rStart, lLocal = lEnd - lStart, nGlobal = nEnd - nStart, mGlobal = mEnd - mStart;\n",
      "  err = MPI_Comm_size(comm, &size); MPI_CHK(err);\n",
      "  err = MPI_Comm_rank(comm, &rank); MPI_CHK(err);\n",
      "  \u001b[37m/* TODO: implement a matrix-vector multiplication on a 2d matrix partition */\u001b[39;49;00m\n",
      "  \u001b[37m/* HINT:\u001b[39;49;00m\n",
      "\u001b[37m   * 1. Use DMVCommGetRankCoordinates2D() to get the coordinates of the current rank in a 2d grid of MPI processes.\u001b[39;49;00m\n",
      "\u001b[37m   * 2. Use the coordinates as colorings to split the communicator into row and column communicators.\u001b[39;49;00m\n",
      "\u001b[37m   * 3. Use MPI_Allgatherv() on the column communicator so that every process has the correct entries to multiply against its local portion of the matrix ([nStart, nEnd]).\u001b[39;49;00m\n",
      "\u001b[37m   *      Look at DenseMatVec_RowPartition_Allgatherv() in dmv_matvec_row.c for an example of using MPI_Algatherv() in this way, but adapt it to the column communicator.\u001b[39;49;00m\n",
      "\u001b[37m   *      You will need to allocate temporary space for the result of the allgatherv, and free it when you are done with it.\u001b[39;49;00m\n",
      "\u001b[37m   * 4. Compute the local portion of the matrix vector product.\u001b[39;49;00m\n",
      "\u001b[37m   * 5. Use MPI_Reduce_scatter() on the row communicator to sum all of the row contributions to vecLeftLocal.\u001b[39;49;00m\n",
      "\u001b[37m   *      Look at DenseMatVec_ColPartition() in dmv_matvec_col.c for an example of use MPI_Reduce_scatter() in this wary, but adapt it to the row communicator.\u001b[39;49;00m\n",
      "\u001b[37m   */\u001b[39;49;00m\n",
      "  DMVCommGetRankCoordinates2D(comm, &num_rows, &row, &num_cols, &col);\n",
      "  err = MPI_Comm_split(comm, row, rank, &rowComm); MPI_CHK(err);\n",
      "  err = MPI_Comm_split(comm, col, rank, &colComm); MPI_CHK(err);\n",
      "    \n",
      "  \u001b[37m// vecRight\u001b[39;49;00m\n",
      "  vecRight = (\u001b[36mdouble\u001b[39;49;00m *) malloc(nGlobal * \u001b[34msizeof\u001b[39;49;00m(*vecRight));\n",
      "  nLocals = (\u001b[36mint\u001b[39;49;00m *) malloc(num_rows * \u001b[34msizeof\u001b[39;49;00m(*nOffsets));\n",
      "  err = MPI_Allgather(&rLocal, \u001b[34m1\u001b[39;49;00m, MPI_INT, nLocals, \u001b[34m1\u001b[39;49;00m, MPI_INT, colComm); MPI_CHK(err);\n",
      "  nOffsets = (\u001b[36mint\u001b[39;49;00m *) malloc((num_rows + \u001b[34m1\u001b[39;49;00m) * \u001b[34msizeof\u001b[39;49;00m(*nOffsets));\n",
      "  nOffsets[\u001b[34m0\u001b[39;49;00m] = \u001b[34m0\u001b[39;49;00m;\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m q = \u001b[34m0\u001b[39;49;00m; q < num_rows; q++) {\n",
      "    nOffsets[q + \u001b[34m1\u001b[39;49;00m] = nOffsets[q] + nLocals[q];\n",
      "  }\n",
      "  \n",
      "  \u001b[37m//Allgatherv\u001b[39;49;00m\n",
      "  err = MPI_Allgatherv(vecRightLocal, rLocal, MPI_DOUBLE, vecRight, nLocals, nOffsets, MPI_DOUBLE, colComm); MPI_CHK(err);\n",
      "  \n",
      "  \u001b[37m//vecLeft\u001b[39;49;00m\n",
      "  \u001b[36mdouble\u001b[39;49;00m *vecLeft;\n",
      "  vecLeft = (\u001b[36mdouble\u001b[39;49;00m *)malloc((mGlobal) * \u001b[34msizeof\u001b[39;49;00m(\u001b[36mdouble\u001b[39;49;00m));\n",
      "    \u001b[37m//outer row, inner column\u001b[39;49;00m\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m q = \u001b[34m0\u001b[39;49;00m; q < mGlobal; q++) { \n",
      "    vecLeft[q] = \u001b[34m0\u001b[39;49;00m;\n",
      "    \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < nGlobal; i++) { \n",
      "      vecLeft[q] += vecRight[i] *  matrixEntries[q*nGlobal + i];\n",
      "    }\n",
      "  }\n",
      "  \u001b[37m// free the vec temp\u001b[39;49;00m\n",
      "  free(vecRight);\n",
      "\n",
      "  \u001b[37m//get the origin , dest pair for send/receive\u001b[39;49;00m\n",
      "  pairOrigin = (rank/num_cols) + (rank%num_cols)*num_rows; \n",
      "  pairDest = row*num_cols + col;\n",
      "    \n",
      "  \u001b[37m//Sendrecv\u001b[39;49;00m\n",
      "  MPI_Sendrecv(&lLocal, \u001b[34m1\u001b[39;49;00m, MPI_INT, pairOrigin, \u001b[34m100\u001b[39;49;00m, &buffsize, \u001b[34m1\u001b[39;49;00m, MPI_INT, pairDest, \u001b[34m100\u001b[39;49;00m, comm, MPI_STATUS_IGNORE);\n",
      "  recvBuf = (\u001b[36mdouble\u001b[39;49;00m *) malloc(buffsize * \u001b[34msizeof\u001b[39;49;00m(*recvBuf));\n",
      "    \n",
      "  \u001b[34mif\u001b[39;49;00m (!recvBuf) MPI_CHK(\u001b[34m1\u001b[39;49;00m);\n",
      " \n",
      "  mLocals = (\u001b[36mint\u001b[39;49;00m *) malloc(num_cols * \u001b[34msizeof\u001b[39;49;00m(\u001b[36mint\u001b[39;49;00m));\n",
      "  err = MPI_Allgather(&buffsize, \u001b[34m1\u001b[39;49;00m, MPI_INT, mLocals, \u001b[34m1\u001b[39;49;00m, MPI_INT, rowComm); MPI_CHK(err);\n",
      "  err = MPI_Reduce_scatter(MPI_IN_PLACE, vecLeft, mLocals, MPI_DOUBLE, MPI_SUM, rowComm); MPI_CHK(err);\n",
      "  err = MPI_Sendrecv(vecLeft, buffsize,  MPI_DOUBLE, pairDest, \u001b[34m100\u001b[39;49;00m, vecLeftLocal, lLocal, MPI_DOUBLE, pairOrigin, \u001b[34m100\u001b[39;49;00m, comm, MPI_STATUS_IGNORE); MPI_CHK(err);\n",
      "    \n",
      "  \u001b[37m//free the temp\u001b[39;49;00m\n",
      "  free(recvBuf);\n",
      "  free(mLocals);\n",
      "  free(nLocals);\n",
      "  free(nOffsets);\n",
      "  free(vecLeft);\n",
      "  err = MPI_Comm_free(&rowComm);\n",
      "  MPI_CHK(err);\n",
      "  err = MPI_Comm_free(&colComm);\n",
      "  MPI_CHK(err);\n",
      "  \u001b[34mreturn\u001b[39;49;00m \u001b[34m0\u001b[39;49;00m;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pygmentize -l c dmv_matvec_2d.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code, `matrixEntries` is a row-oriented 2D array of size `(mEnd - mStart) x (nEnd - nStart)` that represents the submatrix `A[mStart:(mEnd - 1),nStart:(nEnd - 1)]`.  `vecRightLocal` is an array of size `rEnd - rStart` that represents the subvector `x[rStart:(rEnd-1)]`, and `vecLeftLocal` is an array of size `lEnd - lStart` that represents the subvector `b[lStart:(lEnd-1)]`.  You may want to look at the implementations in [dmv_matvec_row.c](./dmv_matvec_row.c) and [dmv_matvec_col.c](./dmv_matvec_col.c), which have the same arguments.\n",
    "\n",
    "When your code is correct, we should be able to run your implementation on different random inputs and get the same results as the other implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o dense_matrix_vector_product\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dense_matrix_vector_product.o dense_matrix_vector_product.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_args.o dmv_args.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv.o dmv.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_global_size.o dmv_global_size.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_layout.o dmv_layout.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_2d.o dmv_matvec_2d.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec.o dmv_matvec.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_col.o dmv_matvec_col.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_row.o dmv_matvec_row.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_offset.o dmv_offset.c\n",
      "mpicc -o dense_matrix_vector_product dense_matrix_vector_product.o dmv_args.o dmv.o dmv_global_size.o dmv_layout.o dmv_matvec_2d.o dmv_matvec.o dmv_matvec_col.o dmv_matvec_row.o dmv_offset.o -lm -lrt\n",
      "Right vector norm: 18.6581\n",
      "Matrix Frobenius norm: 596.735\n",
      "[0] Average time per matvec: 0.000182872\n",
      "Left vector norm: 318.194\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "make dense_matrix_vector_product\n",
    "mpirun -f ${PBS_NODEFILE} -np 8 ./dense_matrix_vector_product -p 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o dense_matrix_vector_product\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dense_matrix_vector_product.o dense_matrix_vector_product.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_args.o dmv_args.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv.o dmv.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_global_size.o dmv_global_size.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_layout.o dmv_layout.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_2d.o dmv_matvec_2d.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec.o dmv_matvec.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_col.o dmv_matvec_col.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_matvec_row.o dmv_matvec_row.c\n",
      "mpicc -I../../utils/Random123/include -I../../utils -g -Wall -O3 -std=c99 -c -o dmv_offset.o dmv_offset.c\n",
      "mpicc -o dense_matrix_vector_product dense_matrix_vector_product.o dmv_args.o dmv.o dmv_global_size.o dmv_layout.o dmv_matvec_2d.o dmv_matvec.o dmv_matvec_col.o dmv_matvec_row.o dmv_offset.o -lm -lrt\n",
      "==== Iteration 1, seed 1572484508 ====\n",
      "== row decomposition ==\n",
      "Right vector norm: 18.8087\n",
      "Matrix Frobenius norm: 593.813\n",
      "[0] Average time per matvec: 0.00015968\n",
      "Left vector norm: 72.5541\n",
      "== column decomposition ==\n",
      "Right vector norm: 18.8087\n",
      "Matrix Frobenius norm: 593.813\n",
      "[0] Average time per matvec: 0.000560246\n",
      "Left vector norm: 72.5541\n",
      "== 2D decomposition ==\n",
      "Right vector norm: 18.8087\n",
      "Matrix Frobenius norm: 593.813\n",
      "[0] Average time per matvec: 0.00041581\n",
      "Left vector norm: 72.5541\n",
      "==== Iteration 2, seed 1572484519 ====\n",
      "== row decomposition ==\n",
      "Right vector norm: 18.2464\n",
      "Matrix Frobenius norm: 587.267\n",
      "[0] Average time per matvec: 0.00016736\n",
      "Left vector norm: 387.29\n",
      "== column decomposition ==\n",
      "Right vector norm: 18.2464\n",
      "Matrix Frobenius norm: 587.267\n",
      "[0] Average time per matvec: 0.000389537\n",
      "Left vector norm: 387.29\n",
      "== 2D decomposition ==\n",
      "Right vector norm: 18.2464\n",
      "Matrix Frobenius norm: 587.267\n",
      "[0] Average time per matvec: 0.000417327\n",
      "Left vector norm: 387.29\n",
      "==== Iteration 3, seed 1572484529 ====\n",
      "== row decomposition ==\n",
      "Right vector norm: 18.3981\n",
      "Matrix Frobenius norm: 598.508\n",
      "[0] Average time per matvec: 0.000450417\n",
      "Left vector norm: 35.9826\n",
      "== column decomposition ==\n",
      "Right vector norm: 18.3981\n",
      "Matrix Frobenius norm: 598.508\n",
      "[0] Average time per matvec: 0.000392235\n",
      "Left vector norm: 35.9826\n",
      "== 2D decomposition ==\n",
      "Right vector norm: 18.3981\n",
      "Matrix Frobenius norm: 598.508\n",
      "[0] Average time per matvec: 0.000583378\n",
      "Left vector norm: 35.9826\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "make dense_matrix_vector_product\n",
    "for i in {1..3}; do\n",
    "  seed=`date +%s`\n",
    "  echo \"==== Iteration $i, seed $seed ====\"\n",
    "  echo \"== row decomposition ==\"\n",
    "  mpirun -f ${PBS_NODEFILE} -np ${PBS_NP} ./dense_matrix_vector_product -p cols -e $seed\n",
    "  echo \"== column decomposition ==\"\n",
    "  mpirun -f ${PBS_NODEFILE} -np ${PBS_NP} ./dense_matrix_vector_product -p rows -e $seed\n",
    "  echo \"== 2D decomposition ==\"\n",
    "  mpirun -f ${PBS_NODEFILE} -np ${PBS_NP} ./dense_matrix_vector_product -p 2d   -e $seed\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（I have seeked help from Zifan and professor on debugging this problem, my version for the last step, is troubled with the segmentation error, so I asked Zifan to check upon my code and helped me on reconstructing part of my previous code.)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Runtime analysis, 2 pts):** Use the $\\lambda + \\mu m$ complexity model of a message and the complexities of collectives that we discussed in class and normalize the cost of a single flop to 1.  Estimate for the 2D decomposition:\n",
    "\n",
    "- $T_{\\text{comp}}$ the local computation time on each process for an $n \\times n$ matrix with $p$ processes,\n",
    "- $T_{\\text{net}}(n \\times n, p)$, the network communication time of the 2D decomposition,\n",
    "- $S(n\\times n, p) = T(n \\times n, 1) / T(n \\times n, p)$, the speedup, and\n",
    "- $\\max_p S(n\\times n, p)$, the maximum speedup over all number of processes.\n",
    "\n",
    "You may assume $\\sqrt{p}$ and $p$ both divide $n$.  In addition to the complexities of collectives that we have discussed in class, you may assume that the complexity of [MPI_Reduce_scatter](https://www.mpich.org/static/docs/latest/www3/MPI_Reduce_scatter.html) the sums vectors of length $mp$ from each process and scatters them across the processes is $\\lambda \\log_2 p + 2 \\mu mp.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose each process p at most responsible $[\\frac{n}{\\sqrt{p}}]$,\n",
    "\n",
    "For $n$ x $n$ matrix, $p$ process, $T_{\\text{comp}} = （n/\\sqrt{p})（n/\\sqrt{p}) = \\frac{n^2}{p}$\n",
    "\n",
    "In the 2D decomposition: \n",
    "\n",
    "We have \n",
    "\n",
    "***All_gather***:  $\\lambda log(p) + \\mu (p-1) \\frac{n}{\\sqrt{p}}$\n",
    "\n",
    "***Sendrecv***: $\\lambda + \\mu  \\frac{n}{\\sqrt{p}}$\n",
    "\n",
    "***Reduce_Scatter*** $\\lambda \\log_2 p + 2 \\mu \\frac{n}{\\sqrt{p}}.$.\n",
    "\n",
    "\n",
    "Overall time is:\n",
    "$T_{net} = \\lambda (\\log_2 p+ \\log{p} + 1) + \\mu ((p-1) \\frac{n}{\\sqrt{p}}+\\frac{n}{\\sqrt{p}}+2\\frac{n}{\\sqrt{p}})$\n",
    "\n",
    "Because, $T(n \\times n, 1) = n^2 $\n",
    "\n",
    "$S(n\\times n, p) = T(n \\times n, 1) / T(n \\times n, p)  = \\frac{n^2}{T_{net}}$\n",
    "\n",
    "$S(n\\times n, p) = T(n \\times n, 1) / T(n \\times n, p)  =  \\frac{n^2}{\\lambda (\\log_2 p+ \\log{p} + 1) + \\mu ((p-1) \\frac{n}{\\sqrt{p}}+\\frac{n}{\\sqrt{p}}+2\\frac{n}{\\sqrt{p}})}$\n",
    "\n",
    "The $\\max_p S(n\\times n, p)$ can be acquired when $T_{net}$ is minimum.\n",
    "\n",
    "We can see that $T_{net}$ is dominated by $\\Theta((\\log{p} )\\frac{n}{\\sqrt{p}})$\n",
    "\n",
    "And $T(n \\times n, 1)= n^2 $,\n",
    "\n",
    "Then $n^2 >C( (\\log{p} )\\frac{n}{\\sqrt{p}})$\n",
    "\n",
    "So we have : $C < = \\frac{(\\log{p} )\\frac{n}{\\sqrt{p}}}{n^2}$ which is the upper limit of $\\max_p S(n\\times n, p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
