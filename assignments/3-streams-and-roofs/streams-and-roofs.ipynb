{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streams and Roofs\n",
    "\n",
    "In this week's assignment we are going to make some roofline diagrams for some $n$-body problems.\n",
    "\n",
    "This week's assignment is meant to be run on a node with a Tesla P100 GPU.\n",
    "\n",
    "A reminder: when you are running a job to complete this week's assignment, you should make sure that you have requested exclusive access to a node, and that you have requested access to all CPU cores of this node.\n",
    "\n",
    "**Due: Thursday, September 12, before class**\n",
    "\n",
    "Let's load in our class module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "module use $CSE6230_DIR/modulefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\n",
      "  1) curl/7.42.1\n",
      "  2) git/2.13.4\n",
      "  3) python/3.6\n",
      "  4) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/jupyter/1.0\n",
      "  5) intel/16.0\n",
      "  6) cuda/8.0.44\n",
      "  7) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/hpctoolkit/2018.18\n",
      "  8) impi/5.1.1.109\n",
      "  9) cse6230/core(default)\n"
     ]
    }
   ],
   "source": [
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that we're running where we expect to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep  8 19:12:33 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   23C    P0    25W / 250W |      0MiB / 16280MiB |      0%   E. Process |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now, about the $n$-body simulations we're going to run: a classical $n$-body simulation has each body, or *particle*, interacting with each other, for $n(n+1)/2$ total interactions.  That hardly matches up to the streaming kernels we've been talking about!  So we're going to simplify a bit.\n",
    "\n",
    "We are going to simulate $n$ infinitesimal particles circling around an infinitely massive sun at the origin.  In this system, the sun is unmoved, and the particles are not affected by each other.\n",
    "\n",
    "We're going to normalize our coefficients and say that each particle is an ordinary differential equation with *six* components: three of position $X=(x, y, z)$ and three of velocity $U=(u, v, w)$.  The position, is changed by the velocity, of course, but the velocity changes under acceleration that depends on position:\n",
    "\n",
    "$$\\begin{aligned} \\dot{X} &= V \\\\ \\dot{V} &= - \\frac{X}{|X|^3}.\\end{aligned}$$\n",
    "\n",
    "To discretize this differential equation, we are going to use a time stepping method called the Verlet leap-frog method, which is good for calculating long simulations of stable orbits.  Given a time step length `dt`, our pseudocode for one time step for one particle looks like the following:\n",
    "\n",
    "1. `X += 0.5 * dt * V`\n",
    "2. `R2 = X . X` (dot product)\n",
    "3. `R = sqrt (R2)`\n",
    "4. `IR3 = 1. / (R2 * R)`\n",
    "5. `V -= X * dt * IR3`\n",
    "6. `X += 0.5 * dt * V`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Assuming `sqrt` and `div` count for one flop each, and assuming `x, y, z` and `u, v, w` are **double-precision** floating point\n",
    "numbers, **estimate the arithmetic intensity of a *particle time step***.  You should ignore the time it takes to load `dt`.  Your answer should have units of flops / byte.  Give your answer in a new cell below this one, and show how you arrived at that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1 Answer:**\n",
    "$$ ArithmeticIntensity = \\frac{\\frac{flops}{kernel}}{\\frac{bytes loaded and stored}{kernel}}$$\n",
    "Units are flop / byte.<br>\n",
    "**In this case:**<br>\n",
    "we could specify `0.5*dt` as a constant variable type double, Thus `0.5*dt` does not count for 1 mul, since when N is very large, the impact of this step will be minimal.\n",
    "all the temporary variables are not needed to write.\n",
    "Thus, optimized write and load shall only takes 6 writes and 6 loads for X,V vectors. \n",
    "1. `X += 0.5 * dt * V`  (matrix scaling, V is 1 x 3 vector, thus each mul takes 3 Flops, in this step, 3 Flops for Muls,3 for add) <br>\n",
    "2. `R2 = X . X` (dot product) (matrix inner product: total 5 Flops :3 for Muls， 3-1=2 for Add) <br>\n",
    "3. `R = sqrt (R2)` (1 sqrt = 1 Flop) <br>\n",
    "4. `IR3 = 1. / (R2 * R)` (1 Mul， 1 Div）<br>\n",
    "5. `V -= X * dt * IR3` （matrix scaling 1x3 = 3 Flops and 1 mul for `dt*IR3` thus total 4 for mul, 3 add)<br>\n",
    "6. `X += 0.5 * dt * V` (matrix scaling 1x3 = 3 Flops for mul, 3 add)<br>\n",
    "**Thus**\n",
    "$$ ArithmeticIntensity = \\frac{\\frac{6+5+1+2+7+6}{kernel}}{\\frac{（6+6）*8bytes}{kernel}}$$\n",
    "Units are flop / byte.<br>\n",
    "$$ ArithmeticIntensity = 27 flops/（12*8bytes）flop/byte $$\n",
    "$$ ArithmeticIntensity = 27 flops/（96bytes）flop/byte $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Using the peak theoretical **double-precision** flop/s of this node (flop/s on the CPUs and GPU combined), calculated the same way as in the last assignment, and reported peak memory bandwidths from the manufacturers, **estimate the system balance of CPUs and the GPU of this node separately**.  Note that the bandwidth estimate from intel will be for one socket (4 cores) with attached memory, and our node has two such sockets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2 Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU(s):                8\n",
      "Thread(s) per core:    1\n",
      "Core(s) per socket:    4\n",
      "Socket(s):             2\n",
      "Sun Sep  8 20:44:16 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   23C    P0    25W / 250W |      0MiB / 16280MiB |      0%   E. Process |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "This nodes has 8 cores: its architecture is (Manufacturer, Product Id) Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz\n",
      "This node has 1 GPUs: its/their architecture is (Manufacturer, Product Id) Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "lscpu | grep -E '^Thread|^Core|^Socket|^CPU\\('\n",
    "nvidia-smi\n",
    "CPU_NAME=`cat /proc/cpuinfo| grep 'model name'|uniq | grep -P -m 1 -o -e \"(?<=model name\\s: ).*\"`\n",
    "CORE_COUNT=`cat /proc/cpuinfo |grep 'model name'|wc -l`\n",
    "GPU_NAME=`nvidia-smi -q|grep 'Product Name'|uniq|grep -o \"Tesla.*\"`\n",
    "GPU_COUNT=`nvidia-smi -q|grep 'Product Name'|wc -l`\n",
    "echo \"This nodes has ${CORE_COUNT} cores: its architecture is (Manufacturer, Product Id) ${CPU_NAME}\"\n",
    "if [[ ! $GPU_COUNT || $GPU_COUNT == 0 ]] ;  then\n",
    "    echo \"This node has no GPUs\"\n",
    "else\n",
    "    echo \"This node has ${GPU_COUNT} GPUs: its/their architecture is (Manufacturer, Product Id) ${GPU_NAME}\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate CPU FLOPS**\n",
    "$$CPU FLOPS = sockets * \\frac{cores}{socket} *\\frac{cycles}{second}*\\frac{FLOPS}{cycle}$$\n",
    "<br>\n",
    "$$CPU FLOPS = 2 *4* 2.60Ghz*16$$\n",
    "<br>\n",
    "$$CPU FLOPS = 332.8 GFlOPS/s$$\n",
    "\n",
    "From https://ark.intel.com\n",
    "Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz\n",
    "Max Memory Bandwidth is 68.3 GB/s， according to the the question, the bandwidth estimate from intel is for one socket, however, we have two sockets.\n",
    "Thus \n",
    "$$CPU machine balance = \\frac{peak flops}{max memory bandwidth}$$\n",
    "<br>\n",
    "$$CPU machine balance = \\frac{332.8 GFlOPS/s}{2*68.3 GB/s}$$\n",
    "<br>\n",
    "$$CPU machine balance = 2.436 FLOP/Byte$$\n",
    "\n",
    "**Calculate GPU FLOPS**\n",
    "$$GPU FLOPS = Texture Units *Clock speed *\\frac{FLOPS}{cycle}$$\n",
    "<br>\n",
    "$$GPU FLOPS = 224*1328Mhz*32$$\n",
    "<br>\n",
    "$$GPU FLOPS = 9.519104TFLOPS/s$$\n",
    "\n",
    "From https://images.nvidia.com/content/tesla/pdf/nvidia-tesla-p100-PCIe-datasheet.pdf\n",
    "Max Memory Bandwidth is 732.2 GB/s\n",
    "$$GPU machine balance = \\frac{peak flops}{max memory bandwidth}$$\n",
    "<br>\n",
    "$$GPU machine balance =\\frac{9.519104TFLOPS/s}{732.2 GB/s}$$\n",
    "<br>\n",
    "$$GPU machine balance =13.00 FLOPS/Byte$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we didn't take the peak flop/s values from the manufacturers at face value, and this week we are not going to take the beak Gbyte/s for granted either.  Last week we used a custom benchmark in our calculations; this week we will use an industry standard: the\n",
    "[STREAM benchmark](https://www.cs.virginia.edu/stream/ref.html).\n",
    "\n",
    "We can run the stream benchmark on the CPUs for this assignment with a makefile target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make runstream STREAM_N=4000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `STREAM_N` argument will control the size of the stream arrays.\n",
    "\n",
    "**Question 3:** Modify the invocation of `make runstreams` by modifying the values of\n",
    "`STREAM_N`, `COPTFLAGS` (optimization flags), `OMP_NUM_THREADS` and/or `OMP_PROC_BIND` (the openMP environment variables) to get the largest streaming bandwidth from main memory that you can for this node.\n",
    "\n",
    "[The OpenMP environment variables were not defined by me in the Makefile: they are environment variables that will be detected by the OpenMP runtime when an OpenMP program begins.  You should put them _before_ the make command, e.g. `OMP_NUM_THREADS=5 make runstream STREAM_N=40000000`]\n",
    "\n",
    "- Follow the directions in the output of the file and make sure you are testing streaming bandwidth from memory and not from a higher level of cache.\n",
    "- You should try to get close to the same bandwidth for all tests:\n",
    "\n",
    "- There are two variables in the openMP environment you should care about, OMP_NUM_THREADS, which is self explanatory, and OMP_PROC_BIND is discussed [here](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html).  **You should try to use as few threads as possible** to achieve peak bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3 Answer:**\n",
    "<br>\n",
    "According to the stream.c instructions, about adjusting stream array size,  \n",
    "<br>\n",
    "Example 2: Two Xeon E5's with 20 MB L3 cache each (using OpenMP)\n",
    " *               STREAM_ARRAY_SIZE should be >= 20 million, giving\n",
    " *               an array size of 153 MB and a total memory requirement\n",
    " *               of 458 MB.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "icc -g -Wall -fPIC -O3 -xHost -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream stream.c -DSTREAM_ARRAY_SIZE=20000000\n",
      "./stream\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 20000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 152.6 MiB (= 0.1 GiB).\n",
      "Total memory required = 457.8 MiB (= 0.4 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 8\n",
      "Number of Threads counted = 8\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 6448 microseconds.\n",
      "   (= 6448 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           67310.8     0.010696     0.004754     0.015570\n",
      "Scale:          67072.0     0.006243     0.004771     0.012148\n",
      "Add:            65636.4     0.012477     0.007313     0.014694\n",
      "Triad:          65997.9     0.012740     0.007273     0.016249\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "OMP_NUM_THREADS=8 OMP_PROC_BIND=spread make runstream STREAM_N=20000000 COPTFLAGS='-O3 -xHost' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What does `OMP_PROC_BIND=close` mean, and why is it a bad choice, not just for this benchmark, but for any streaming kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4 Answer:**\n",
    "<br>\n",
    "Setting OMP_PROC_BIND=close means Binding threads close to the master thread while still distributing threads for load balancing，the thread assignment also goes successively through the available places. \n",
    "<br>\n",
    "Because with OMP_PROC_BIND=close, the threads are not assigned evenly over sockets, instead, it will assign threads successively on a single socket, this is not good for moving data, considering the locality favored strategy to achieve high performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** I've modified the benchmark, calling it `stream2.c`.  Here's the difference, it's one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267d266\n",
      "< #pragma omp parallel for\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "diff stream.c stream2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your options for `runstream` to `runstream2` below.  The reported results should be different: why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "icc -g -Wall -fPIC -O3 -xHost -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream2 stream2.c -DSTREAM_ARRAY_SIZE=20000000\n",
      "./stream2\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 20000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 152.6 MiB (= 0.1 GiB).\n",
      "Total memory required = 457.8 MiB (= 0.4 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 8\n",
      "Number of Threads counted = 8\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 8219 microseconds.\n",
      "   (= 8219 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           31974.1     0.010722     0.010008     0.014600\n",
      "Scale:          31695.5     0.010141     0.010096     0.010185\n",
      "Add:            34651.7     0.013973     0.013852     0.014025\n",
      "Triad:          34473.1     0.014006     0.013924     0.014107\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "OMP_NUM_THREADS=8 OMP_PROC_BIND=spread make runstream2 STREAM_N=20000000 COPTFLAGS='-O3 -xHost' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5 Answer:**\n",
    "<br>\n",
    "The output report for stream2.c showing the bandwidth is **half** of the output report from stream.c\n",
    "<br>\n",
    "The difference between the stream2.c and stream.c is that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Now we're going to run stream benchmarks for the GPU.  As above, modify the array size until you believe you are testing streaming bandwidth from memory and not from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "nvcc -ccbin=icpc -lineinfo -Xcompiler '-fPIC' -O -o streamcu stream.cu -DSTREAM_ARRAY_SIZE=40000000\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "./streamcu\n",
      "-------------------------------------------------------------\n",
      "CSE6230 CUDA STREAM based on version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "Device Number: 0\n",
      "  Device name: Tesla P100-PCIE-16GB\n",
      "  Memory Clock Rate (KHz): 715000\n",
      "  Memory Bus Width (bits): 4096\n",
      "  Peak Memory Bandwidth (GB/s): 732.160000\n",
      "\n",
      "Ordinal of GPUs requested = 0\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 2826 microseconds.\n",
      "   (= 2826 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:          539243.6     0.001189     0.001187     0.001190\n",
      "Scale:         538702.5     0.001189     0.001188     0.001190\n",
      "Add:           553932.0     0.001734     0.001733     0.001735\n",
      "Triad:         553627.4     0.001735     0.001734     0.001736\n",
      "-------------------------------------------------------------\n",
      "Failed Validation on array a[]\n",
      "        Expected  : 46132031250000003072.000000 \n",
      "        Observed  : 46132031249951989760.000000 \n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "OMP_NUM_THREADS=8 OMP_PROC_BIND=spread make runstreamcu STREAM_N=40000000 COPTFLAGS='-O3 -xHost' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (2 pts):** This is final time we're running a stream benchmark, I promise.  This benchmark is also for the GPU, but instead of the arrays originating in the GPUs memory, they start on the CPUs memory, and must be transfered to the GPU and back.  This mimics a common design pattern when people try to modify their code for GPUs: identify the bottleneck kernel, and try to \"offload\" it to the GPU, where it will have a higher throughput (once it get's there).  You don't have to modify this run, I just want you to see what bandwidths it reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -lineinfo -Xcompiler '-fPIC' -O -o streamcu2 stream2.cu -DSTREAM_ARRAY_SIZE=1000000\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "./streamcu2\n",
      "-------------------------------------------------------------\n",
      "CSE6230 CUDA STREAM based on version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 1000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 7.6 MiB (= 0.0 GiB).\n",
      "Total memory required = 22.9 MiB (= 0.0 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "Ordinal of GPUs requested = 0\n",
      "  Device name: Tesla P100-PCIE-16GB\n",
      "  Memory Clock Rate (KHz): 715000\n",
      "  Memory Bus Width (bits): 4096\n",
      "  Peak Memory Bandwidth (GB/s): 732.160000\n",
      "\n",
      "-------------------------------------------------------------\n",
      "1.000000 2.000000 0.000000\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 1348 microseconds.\n",
      "   (= 1348 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:            6182.9     0.002605     0.002588     0.002635\n",
      "Scale:           6182.3     0.002606     0.002588     0.002625\n",
      "Add:             6182.1     0.003899     0.003882     0.003914\n",
      "Triad:           6192.0     0.003897     0.003876     0.003913\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-12 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make runstreamcu2 STREAM_N=1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the three peak bandwidths that we have *computed* (not the reported values from question 2) -- CPU, GPU with arrays on the GPU, and GPU with arrays on the CPU -- and with the theoretical peak flop/s for the CPU and GPU, compute *effective system balances* and create a plot with rooflines for all three balances overlayed.\n",
    "\n",
    "- The y axis should be absolute Gflop/s, not relative, so we can compare them, and should be labeled \"Gflop/s\"\n",
    "- Label with roofline goes with which balance: \"CPU\", \"GPU\", \"CPU->GPU->CPU\"\n",
    "- The x axis should be in units of \"double precision flops / byte\"\n",
    "\n",
    "Save your plot as the jpg `threerooflines.jpg` so that it can embed in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three rooflines](./threerooflines.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (2 pts):** Remember those particles all the way back in question 1?  Your arithmetic intensity estimate could be placed on the roofline plot for the CPUs, and you could make a judgement about whether the kernel is compute bound or memory bound.\n",
    "\n",
    "Now let's put it to the test.  The `make runcloud` target simulates `NPOINT` particles orbiting the sun for `NT` time steps.  Because these particles are independent, you can optionally \"chunk\" multiple time steps for each particle independent of the other particles.  Doing this reduces the number of memory accesses per flop:  each particle stays in register for `NCHUNK` time steps.\n",
    "\n",
    "Do your best to optimize the throughput of the simulation both in the limit of few particles and many time steps, and in the limit of many particles and few time steps.\n",
    "Do that by modifying the commands below.\n",
    "\n",
    "- Make the simulations each run about a second\n",
    "- Do your best to optimize the compiler flags and the runtime (openMP) environment\n",
    "\n",
    "Using the outputs of those runs, estimate the floating point efficiency of our particle-time-step kernel: compare the peak flop/s of the CPU, to the product of particle time steps per second and your estimate of the flops per particle time step. and divide by the throughput of particle time steps per second.  Give that effective arithmetic intensity below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runcloud NPOINT=64 NT=1000000 NCHUNK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runcloud NPOINT=6400000 NT=100 NCHUNK=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
