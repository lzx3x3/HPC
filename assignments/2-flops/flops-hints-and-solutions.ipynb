{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Performance Metrics & First Week Flop/s\n",
    "\n",
    "This assignment has some questions that you need to answer with text, and some code that you need to write.\n",
    "\n",
    "You should put all of you textual answers in this notebook: `Insert->Insert Cell Below` to create a new cell below\n",
    "the question, and `Cell->Cell Type->Markdown` to make it a cell for entering text.\n",
    "\n",
    "You will test your code on the compute nodes of pace-ice, and that it also where we will evaluate it.\n",
    "Please complete the text portions when you are logged into a head node working locally, and leave the compute nodes for when you actually need them.\n",
    "\n",
    "**Due: Tuesday, September 2, 9:30 am**\n",
    "\n",
    "**Total: 10 pts + 2 bonus pts (1 for working on a node with GPUs, 1 for optimizing the flop/s code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "In class we talked about the _strong-scaling efficiency_ of a parallel algorithm / machine pair: $H_f(P) = T_f(1) / (P T_f(P))$.\n",
    "\n",
    "We then talked about the _weak-scaling efficiency_ of algorithm $f$ that can be applied to different problem sizes $N$: $E_f(N,P) = T_f(N/P,1) / T_f(N,P)$.\n",
    "\n",
    "The question came up of how they are related to each other.\n",
    "\n",
    "First, the notion of strong scaling doesn't have a concept of problem size, so let's add it: let's define\n",
    "\n",
    "$$H_f(N,P) = T_f(N,1) / (P T_f(N,P)).$$\n",
    "\n",
    "This is simply strong-scaling efficiency for each problem instance individually.\n",
    "\n",
    "**Question 1 (1 pt):** Show that the relative order of strong and weak scaling efficiency (Whether $H_f(N,P) < E_f(N,P)$ or $E_f(N,P) < H_f(N,P)$) can be related to the efficiency of the serial algorithm, that is, whether $T_f(N,1)$ as a function of $N$ exhibits superlinear or sublinear behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACE-ICE\n",
    "\n",
    "**Head node exercise 1 (1 pt):** What command should you run from a head node to see a list of all the compute nodes in `coc-ice` and their availability? [Resource for this question: the [orientation slides](http://pace.gatech.edu/sites/default/files/pace-ice_orientation_2.pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- Put your command there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out: open up this notebook on a head node and compare the list you get to the [orientation slides](http://pace.gatech.edu/sites/default/files/pace-ice_orientation_2.pdf).  You'll see that it has grown, and they haven't updated the orientation slides.  We'll just have to find out what all these new nodes are for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A word on running jupyter on pace-ice:\n",
    "\n",
    "As we discussed in class, screen refresh can be a bit laggy if you try to run a jupyter notebook through a browser opened on the head node or a compute node.  See the [guide](../../notes/logistics/compute-node-notebook.ipynb) in the notes for instructions on runnin the jupyter server on the compute nodes and the browser on your own computer.  You don't have to work directly in the notebook: you can work on you answers in the terminal, and then paste them into the notebook, as long as you're confident that they are correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Head node exercise 2 (1 pt):** From the output of the above answer, you can probably see that we have a few different types of nodes to work with.  Fill in the blanks in the list below, describing the _properties_ of the different types. [Command line tools you might want to use: `qnodes`, `grep`, `sort`]\n",
    "\n",
    "1. #__ nodes with __ CPU core(s) and no GPUs\n",
    "2. #__ nodes with __ CPU core(s) and #__ GPU(s) of type NVIDIA Tesla ____.\n",
    "3. #__ nodes with __ CPU core(s) and #__ GPU(s) of type NVIDIA Tesla ____ (this group includes `rich133-s42-21.pace.gatech.edu`, even though that the type of GPU is not listed for this node)\n",
    "5. One node (`rich133-s30-20.pace.gatech.edu`) with __ CPU core(s) (currently offline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Head node exercise 3 (1 pt):** For the next questions, I need you to log in to compute nodes to find out about them, but you need to be able to specify which type of compute nodes you are accessing.\n",
    "\n",
    "For each of the types of nodes 1, 2, and 3 in the question above, give me a `qsub` command to start a `jupyter_notebook_script.sh` job on that type of node, with the following requirements:\n",
    "\n",
    "* The job should give you exclusive access to one node and all its cores and devices.\n",
    "* The job should begin in the CSE6230 directory.\n",
    "* The job should end after 30 minutes.\n",
    "\n",
    "[Resources: [compute-node-notebook.ipynb](../../notes/logistics/compute-node-notebook.ipynb), [orientation slides](http://pace.gatech.edu/sites/default/files/pace-ice_orientation_2.pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the qsub command for type 1 in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the qsub command for type 2 in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the qsub command for type 3 in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we got to work with?\n",
    "\n",
    "Now, we need to switch from a notebook running on the head node to one running on a compue node, so `File->Save and Checkpoint` this notebook and `File->Close and Halt` it.  (Now would also be a good time to `git add` and `git commit` changes to this file.)  Use one of your ineractive job scripts to connect to a compute node and run the notebook there.\n",
    "See you on the other side!\n",
    "\n",
    "---\n",
    "\n",
    "Okay, you're running on the compute node.\n",
    "\n",
    "**Compute node exercise 1 (2 pt):** Using bash scripting (`awk`, `grep`, `sed`) or any other tool you like (you could, e.g., write a python script in a separate file and call it, as long as you `git add` it), set the variables in the cell below so that the printout that follows is correct.  You script should be correct on any type of compute node.\n",
    "\n",
    "Resources: the file `/proc/cpuinfo`, the utility `nvidia-smi`; if you are very new to using a shell command line and the utilities that go with it in linux, please look at the [training slides on Linux](https://pace.gatech.edu/training) from PACE.\n",
    "\n",
    "Note: when you run a command in backticks, you can assign its value to a variable like\n",
    "\n",
    "```bash\n",
    "MY_FILES=`ls -al`\n",
    "```\n",
    "\n",
    "Also note: when ever you encounter a new program or utility `AAA` that you've never used before, `man AAA` or `AAA --help` are the first places to go if you want to know what different command line flags do.\n",
    "\n",
    "Some [one-liners](https://en.wikipedia.org/wiki/One-liner_program) that you may find useful:\n",
    "\n",
    "* `grep -P -m 1 -o -e \"(?<=XXX\\s: ).*\" YYY`: look in file `YYY` for the string \"XXX   : \" (with an arbitrary number of spaces between `XXX` and `:`) and print what comes after that on the line\n",
    "* `wc -l YYY` counts the number of lines in file `YYY`\n",
    "* most command line utilities that read files can also read the output of a previous command with a pipe `|`, for example to count the number of files in a directory:\n",
    "\n",
    "```bash\n",
    "ls -al | wc -l\n",
    "```\n",
    "\n",
    "* `grep -c \"ZZZ\" YYY` count the number of times the string `ZZZ` occurs in file `YYY`\n",
    "* Nodes without GPUs won't have the `nvidia-smi` utility.  You can tell when a utility is unavailable if `which AAA` returns an error.  If you want to write a one-liner that only runs command `AAA` when `nvidia-smi` when it's available, you can do that like this:\n",
    "\n",
    "```bash\n",
    "(which nvidia-smi &> /dev/null) && (AAA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_NAME=\n",
    "CORE_COUNT=\n",
    "GPU_NAME=\n",
    "GPU_COUNT="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"This nodes has ${CORE_COUNT} cores: its architecture is (Manufacturer, Product Id) ${CPU_NAME}\"\n",
    "if [[ ! $GPU_COUNT || $GPU_COUNT == 0 ]] ;  then\n",
    "    echo \"This node has no GPUs\"\n",
    "else\n",
    "    echo \"This node has ${GPU_COUNT} GPUs: its/their architecture is (Manufacturer, Product Id) ${GPU_NAME}\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute node exercise 2 (1 pt):** After you have logged out of the compute node, use whatever resources published on the web you can find to estimate the peak _single precision_ (aka FP32) flop/s of this node (you only need to do this step for one of the types of nodes, not all of them).\n",
    "\n",
    "[Resources: [ark.intel.com](https://ark.intel.com), [wikipedia:FLOPS](https://en.wikipedia.org/wiki/FLOPS), [wikichip](https://en.wikichip.org), our notebook on [processors](../../notes/processors/processors-alone.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen node type __ .  The peak flop/s for this node is ____ gigaflop/s.  Here is how I calculated that:\n",
    "\n",
    "(calculation goes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Count the number of cores, `CPU_COUNT` $N_C$.  Using the `CPU_NAME`, go to the [Intel website](https://ark.intel.com) to find the frequency $\\theta$ (cycles per second) for this type of chip (it may report \"base\" and \"max turbo\" frequency, so we could have $\\theta_{\\text{base}}$ and $\\theta_{\\text{max}}$ ), as well as the \"code name\" (such as \"Broadwell\") that is the architecture of the cores.  The question is how many flops each core of that architecture can compute per cycle, $W_{\\text{core}}$, which we would then multiply by the core count and frequency to get the peak flop/s: $$F_{\\text{peak}}= W_{\\text{core}} * \\theta_{\\text{max}} * N_C.$$\n",
    "\n",
    "I can tell you that all of the CPUs have the [AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#AVX2) vector instruction set, which means that their vector registers have a width of 256 bits = 32 bytes.  A single precision number is 32 bits = 4 bytes, meaning that a vector register can work on 8 single precision operations per core.  A fused multiply-add operation counts as two flops in a single instruction, so a vectorized fused multiply-add computes 8 * 2 = 16 flops.  Looking at the [Intel intrinsics website](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#expand=2549,2185,2553&techs=FMA) we can find how many functional units can compute the vectorized instructions for our architecture (e.g. Broadwell) by looking at the inverse of the throughput.  Throughput is \"cycles per instruction\", we want \"instructions per cycle\".\n",
    "\n",
    "**Solution without showing work**:\n",
    "\n",
    "Type 1: CPUs can achieve 2.15 teraflop/s at base clock frequency, 3.0 teraflop/s at max clock frequency.\n",
    "\n",
    "Type 2: CPUs can achieve 921 gigaflop/s at base clock frequency, 1.38 teraflop/s at max clock frequency; GPUs can achieve 8.58 teraflop/s at base clock frequency, 10.1 teraflop/s at max clock frequency.\n",
    "\n",
    "Type 3: CPUs can achieve 666 gigaflop/s at base clock frequency, 819 gigaflop/s at max clock frequency; GPUs can achieve 9.52 teraflop/s at base clock frequency, 10.6 teraflop/s at max clock frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flop/s fever\n",
    "\n",
    "We've got to scratch that itch: we just want to go fast.  Okay, let's get it out of our system, and we'll look at more practical computations in future assignments.\n",
    "\n",
    "You should choose one of the node types for this task.  Because this is more complex if multiple devices are involved\n",
    "**1 bonus point** is earned for choosing a node with GPUs.\n",
    "\n",
    "**Compute node exercise 3 (2 pts):** The command below will compile and runs essentially the following computation:\n",
    "\n",
    "```C\n",
    "for (i = 0; i < Nh; i++) { /* this loop will run on the \"host\" (CPUs) */\n",
    "  for (j = 0; j < T; j++) {\n",
    "    ah[i] = ah[i] * b + c;\n",
    "  }\n",
    "}\n",
    "\n",
    "for (i = 0; i < Nd; i++) { /* this loop will run on the \"device\" (GPUs) */\n",
    "  for (j = 0; j < T; j++) {\n",
    "    ad[i] = ad[i] * b + c;\n",
    "  }\n",
    "}\n",
    "```\n",
    "And it will report the flop/s for the whole calculation.\n",
    "\n",
    "`Nh` array entries will be on the host and `Nd` entries will be on each of the devices.  Try to find values of `Nh`, `Nd`, and `T`, and (optionally) compiler optimization flags that give you the highest flop/s.  Things to consider:\n",
    "\n",
    "- Try to make your whole computation run for about a second.\n",
    "- The time reported is the maximum time for any device: if one sits idle while the other finishes, it will rob you of flop/s.\n",
    "- I suggest looking at one type of device at a time: set one of `Nh` or `Nd` to zero.  Once you've found your best flop/s for that device, optimize the other, and then try to strike a balance.\n",
    "- Experiment with the merits of putting more weight on `Nh` and `Nd` vs more weight on `T`.\n",
    "  Try to use **Little's Law** to make sure that you have enough parallelism to keep the pipelines filled.\n",
    "- You can also choose to pass the option `Bs=X` to control the thread block size for the GPU, where `X` is a power of 2 between 64 and 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make run_fma_prof Nh=256 Nd=256 T=256 COPTFLAGS='-O -xHost' CUOPTFLAGS='-O' # modify this for peak flop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Node Exercise 4 (Bonus 1 pt):** Now let's see if we can make any transformations to the code to make a difference.\n",
    "\n",
    "We will run the same program, but with fused multiply add loops that you have tried to optimize.  You should edit the files\n",
    "`fma_loop_host_opt.cu` and/or `fma_loop_dev_opt.c`: they start out exactly the same as the reference implementations used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff fma_loop_host.c fma_loop_host_opt.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff fma_loop_dev.cu fma_loop_dev_opt.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can exploit vectorization, instruction level parallelism, and/or loop transformations to get a boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make run_fma_prof_opt Nh=256 Nd=256 T=256 COPTFLAGS='-O -xHost' CUOPTFLAGS='-O' # modify this for peak flop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** \n",
    "\n",
    "**CPUs**: Using unroll and jam (see [these slides](https://www.cs.colostate.edu/~mstrout/CS553/slides/lecture20.pdf) for an in depth description) with an unroll length of 80 estimated by using **Little's law** product of the number of vectorized instructions per core of 16 = 8 (vectorized instructions per FPU) * 2 (FPUs per core).  I made sure there were enough array entries to guarantee every CPU would have enough array entries to fill the unrolled loop at least one: to be safe, I set `Nh` to be 160 * $N_C$ (the number of CPU cores).  I then chose a value of $T$ that made the whole computation run for about a second.  I was able to achieve:\n",
    "\n",
    "- 2.5 teraflop/s on the CPUs of Type 1 nodes (116% of base clock, 83% of max clock) using\n",
    "`Nh=$(( 160*28 ))`, `T=256000000`\n",
    "- 995 gigaflop/s on the CPUs of Type 2 nodes (108% of base clock, 72% of max clock) using `Nh=$(( 160*12 ))`, `T=256000000`\n",
    "- 710 gigaflop/s on the CPUs of Type 3 nodes (107% of base clock, 87% of max clock) using\n",
    "`Nh=$(( 160*8 ))`, `T=256000000`\n",
    "\n",
    "**GPUs**:\n",
    "\n",
    "On the GPUs, I unrolled the `T` loop (which reduces the number of comparisions that occur).  For type 2 GPUs, I found that I needed a block size of at least 128 to achieve full occupancy, but for type 3 GPUs I found that a block size as low as 32 (the number of threads in a warp) was enough to I counted the number of FP32 \"CUDA Cores\", and multiplied that by an estimate of the latency of a fused multiply-add operations (Little's law), to figure out how large the array had to be fill the pipeline on each CUDA core.\n",
    "\n",
    "- 5.7 teraflop/s on the GPUs of Type 2 nodes (66% of base clock, 56% of max clock) using `Nd=$(( 2880 * 2 * 64))` `Bs=192` and `T=6400000`\n",
    "- 8.5 teraflop/s on the GPUs of Type 3 nodes (87% of base clock, 80% of max clock) using `Nd=$(( 3584 * 32 ))` `Bs=64` and `T=32000000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting this work\n",
    "\n",
    "**Workstation exercise 1 (1 pt):** When you have completed the rest of this assignment, `git add` the changes to this file, the source files you modified, and any scripts you added, and `git commit` them.  Having commited your changes, you should `git push` them to the private repository that you have on `github.gatech.edu`.\n",
    "\n",
    "Our TA Han Sol Suh will email each of you a individualized [deploy key](https://developer.github.com/v3/guides/managing-deploy-keys/) that will allow him to read the contents of your repository.  \n",
    "\n",
    "**Assignments need to be formally submitted to canvas,** but the totality of your submission on canvas should be a git revision hash or branch name indicating the version of your repository we should use to grade the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
