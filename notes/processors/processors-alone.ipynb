{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Processors Alone\n",
    "\n",
    "![](images/cpu.jpg)\n",
    "\n",
    "$$\\Huge\\color{blue}{\\text{Registers}}$$\n",
    "\n",
    "$$\\Huge\\color{red}{\\text{Scheduler}}$$\n",
    "\n",
    "$$\\Huge\\color{green}{\\text{Functional Units}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When introducing OpenMP (which we will look at in more depth later in the class), it's typical to start with a simple example of how easy it makes it to parallelize your code:\n",
    "\n",
    "```C\n",
    "/* We added one pragma and it's parallel! */\n",
    "#pragma omp parallel for\n",
    "for (i = 0; i < N; i++) {\n",
    "  A[i] = func (b[i], c[i]);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In fact, parallelizing you code can be even simpler, you don't even have to change it.\n",
    "\n",
    "You simply have to change from this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $CSE6230_DIR/assignments/2-flops\n",
    "cc -g -c -std=c99 -o fma_loop_host.o fma_loop_host.c -O0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc -g -c -std=c99 -o fma_loop_host_opt.o fma_loop_host.c -O3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By asking the compiler to try its best to optimize my code, it is able to exploit parallelism within the CPU core, even for my serial program.\n",
    "\n",
    "---\n",
    "\n",
    "Questions we'd like to answer today:\n",
    "\n",
    "- What kind of parallelism is available in a single core, and how much of it?\n",
    "- How can I exploit it?\n",
    "- Can all applications exploit it?\n",
    "- How can I make the compiler do the work for me?\n",
    "- How does the parallelism on a CPU core compare to the parallelism in a GPU?\n",
    "\n",
    "\n",
    "Tools we will use today:\n",
    "\n",
    "- Code compilers, like `cc` above, focusing on their optimization options.  `cc` is typically an alias for a major compiler (you can typically run `man CC` or `CC --help` to get big lists of optimization and other options)\n",
    "  * GNU `gcc`\n",
    "  * LLVM `clang`\n",
    "  * NVIDIA `nvcc`\n",
    "  * Vendor specific (like Intel `icc`)\n",
    "  (focus on their optimization options)\n",
    "- Code decompilers and diagnostics (to see what the heck compilers are doing)\n",
    "- Hardware counters for things that happen in the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For many, many applications, optimal performance can't be achieved by optimizing just the processor's performance alone: we have to optimize it's interactions with the memory system. That is why I'd like to finish the \"Processors alone\" module today.\n",
    "\n",
    "Luckily, optimizing the processor in isolation is something that compilers are quite good at.\n",
    "\n",
    "If you have to take away one keep concept today, it is **Little's law**, which will often tell you how to structure your code to set the compiler up for success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall the end stage of compilation we discussed in the first lecture: machine code\n",
    "\n",
    "When I compiled `fma_loop_host.o` above, it created a file with those instructions.  It's encoded in binary, so opening it up in a text editor won't tell us much, but I can still find out what those machine code instructions are by decompiling the binary into assembly language.  The utility that let's me do that is called `objdump`.\n",
    "\n",
    "(We haven't talked about how CUDA code is different, but for now let's just mention that it comes with its own decompiler: `cuobjdump`)\n",
    "\n",
    "Here is the entirety of `fma_loop_host.c` from assignment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat fma_loop_host.c | pygmentize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objdump -Sd fma_loop_host.o | pygmentize -l c-objdump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that, as illegible as this is, it would be much worse if we didn't have source code interspersed with instructions.  You should always compile C code with `-g` and CUDA with ~~`-G`~~ `-lineinfo` [`-G` always turns off optimizations]  for this reason and others.)\n",
    "\n",
    "For our purposes, this assembly code has three types of instructions:\n",
    "\n",
    "- Instructions that take **registers** as inputs (those things that are addressed like `%rax` and `%rbp`) and\n",
    "  write their outputs over the locations of their inputs.  Examples are floating point operations like `mulss` (multiply two single precision numbers together), integer operations like `addl` (add two 32-bit integers), and logical operations like `cmp` (determine if one integer is less than another and write the output to a special register).\n",
    "\n",
    "  * In hardware, registers are data locations in a register file: the storage closest to the execution units.\n",
    "    Register space is quite dear, so to reflect that, most instruction sets have a limited number of registers (see\n",
    "    e.g. the wikipedia page for the [AVX512](https://en.wikipedia.org/wiki/AVX-512#Extended_registers) instruction \n",
    "    set.).  When a thread has too many computations to keep track of, data that would otherwise be stored in a register is *spilled* to memory, which slows things down.  I mention all of this just to say that one things compilers are trying to do is figure out how to squeeze your complex instructions into the limited scratchpad space provided by the registers.\n",
    "    \n",
    "- Instructions that load and store data from memory like `mov`: we're not going to talk about open can of worms today.\n",
    "\n",
    "- Branching instructions that control the flow of instructions like `jl` (jump to a given code location based on the outcome of a comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for our simple purposes today, a **thread** is:\n",
    "\n",
    "- a stream of instructions, with\n",
    "- a limited set of registers as a workspace for partial computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a thread is executed\n",
    "\n",
    "(This is a simplification of the [classic RISC pipeline](https://en.wikipedia.org/wiki/Classic_RISC_pipeline))\n",
    "\n",
    "1. An instruction like `add    %rdx,%rax` is:\n",
    "\n",
    "  1. *fetched* from the instruction queue, \n",
    "  2. *decoded* (it's operation and register inputs / outputs are identified), \n",
    "  3. **executed** (the part we care about), and\n",
    "  4. *written back* to registers\n",
    "  \n",
    "2. Move to the next instruction and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining\n",
    "\n",
    "If a *cycle* is the smallest unit of time of a processor, and an instruction has multiple steps (each step takes a cycle), does that means that an instruction takes multiple cycles?\n",
    "\n",
    "Yes!  Let's say $k$ cycles.\n",
    "\n",
    "Does that mean that a processor takes $kN$ cycles to complete $N$ instructions?\n",
    "\n",
    "No! Instructions are **Pipelined:**\n",
    "\n",
    "![pseudorisc pipeline](./images/pipeline-1.jpg)\n",
    "\n",
    "The key thing to understand about pipelined operations:\n",
    "\n",
    "**The results of an operation can't be inputs to another operation until they exit\n",
    "the pipeline.**\n",
    "\n",
    "Any cycle of a pipeline when there isn't a new input is a *bubble*.\n",
    "\n",
    "The *efficiency* (work / cycle) of your pipelined algorithm is the *fraction of non-bubble cycles*.\n",
    "\n",
    "**A fully efficient pipelined algorithm has at least $k$ concurrent independent operations at any point in time, where $k$ is the depth of the pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I think that the way that many diagrams show pipelined instructions (time axis horizontal, data axis vertical, instructions labeled) is not helpful, because the \"pipe\" in the pipeline is always moving, and because the diagram gets larger in both dimensions as time goes on.  I prefer to have *instructions* on the vertical axis and *data* labeled on the diagram, because that way the diagram only grows on one axis, and each column looks like a time slice of the pipeline)\n",
    "\n",
    "![pipeline](./images/pipeline-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines and branching\n",
    "\n",
    "Even in our simple program, we saw that my nice clean breakdown of register-register instructions and memory instructions wasn't respected: some instructions like `mulss  -0x24(%rbp),%xmm0` combine a memory access\n",
    "(`-0x24(%rbp)` accesses a memory location stored in `%rbp`, offset by a certain amount).\n",
    "\n",
    "More complex instructions require more decoding: the pipeline of operations before **execute** is quite long on a modern CPU.\n",
    "\n",
    "![long pipeline](./images/pipeline-3.jpg)\n",
    "\n",
    "That is why branching instructions are hard to combine with pipelined execution.  We don't know which instruction\n",
    "should go into the pipeline, it depends on the output of a computation like a comparison.  The CPU could:\n",
    "\n",
    "- Hold up everything to wait until it is known which branch to take (always stall the pipeline, bad)\n",
    "- Try to *predict* which branch will be taken an keep feeding the pipeline with that branch (bad when there is a *misprediction*)\n",
    "\n",
    "Branch prediction is a complicated, sophisticated thing on modern CPUs.  In your programming, you should assume the following:\n",
    "\n",
    "- Computers are good are recognizing patterns: there is a branch in every loop of a for-loop, but if you keep looping back, it will eventually start predicting that is the branch to take, and a branch will be a neglible part of the execution time.  For loops with known bounds can also be **unrolled** meaning copy-pasted the right number of times with no branching at all.\n",
    "\n",
    "```C\n",
    "for (int i = 0; i < N; i++) { /* if N = 10000000000, branch prediction will almost always be right */\n",
    "    /* ... */\n",
    "}\n",
    "```\n",
    "\n",
    "```C\n",
    "for (int  i = 0; i < 8; i++) {\n",
    "  /* If the bound is known at compile time, the loop can be unrolled with no branching */\n",
    "  /* ... */\n",
    "}\n",
    "```\n",
    "\n",
    "```C\n",
    "/* You can give the compiler hints about how you want to break up a loop in to unrolled sections,\n",
    "   reducing the number of branches */\n",
    "#pramga unroll(8)\n",
    "for (int i = 0; i < N; i++) {\n",
    "    /* ... */\n",
    "}\n",
    "```\n",
    "\n",
    "- If your branching has no patterns, then you should expect lots of branch misprediction: the instruction pipeline \n",
    "  has to be cleared out, leading to a stall in your code *proportional in length to the pipeline depth* (~10-20 cycles)\n",
    "  \n",
    "- Branch misprediction is the kind of hardware event that can be counted by a performance counter like `perf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $CSE6230_DIR/assignments/2-flops\n",
    "make run_fma_prof PERF=\"perf stat -v\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing instructions\n",
    "\n",
    "Like I said, the depth of the pipeline before and after execution really only affects us when there is branching.  Let's talk about *execute*:\n",
    "\n",
    "- Different types of instructions are executed on different *functional units*:\n",
    "\n",
    "  - *ALU*: arithmetic and logic unit\n",
    "  - *FPU*: floating point unit\n",
    "  - etc.\n",
    "  \n",
    "See, e.g., the [Kaby Lake](https://en.wikichip.org/wiki/intel/microarchitectures/kaby_lake) diagram from Wikichip that we saw in the first lecture.  This is what the cartoon at the top of the lecture is supposed to be a simplification of.\n",
    "\n",
    "![Kaby Lake](./images/kabylake.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superscalarity\n",
    "\n",
    "There are multiple functional units in a processor.  In the pipeline diagrams we've seen so far, there is only one `execute` instruction happening per cycle.  That would mean that only one functional units is called on per cycle, leaving the others idle.  Is that a waste of resources?\n",
    "\n",
    "It is, the diagrams are wrong! Modern CPUS are **superscalar:** there are multiple instruction pipelines that can happen at once.\n",
    "\n",
    "![superscalar pipeline](./images/pipeline-super.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we exploit superscalarity?\n",
    "\n",
    "- Some combination of a smart **scheduler,** which is able to\n",
    "  1. Look ahead several instructions,\n",
    "  2. Identify *independent* operations, and\n",
    "  3. Reorder for concurrent independence\n",
    "  \n",
    "- And a smart **compiler**, which\n",
    "  1. Knows the functional units that are available\n",
    "  2. Knows the amount of register space available and the superscalar factor, and\n",
    "  3. Tries to reorder and change which registers are used to solve the\n",
    "     optimal scheduling problem\n",
    "     \n",
    "In almost all cases, the compiler is better than you are at this: don't try to out think it.\n",
    "\n",
    "If you think the compiler is getting it wrong:\n",
    "\n",
    "- Use the decompiler to see what it's doing\n",
    "- Use *optimization reports* (like Intel `-qopt-report=5`) to ask the compiler to tell you what it's doing\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also exploit superscalarity with multiple threads\n",
    "\n",
    "When one thread has a pipeline stall, another can be issuing instructions.\n",
    "\n",
    "- If there is hardware support for multiple threads, that means they can both have their registers in the register file at the same time, and the scheduler can switch between them.  If there is OS support for multiple threads, that means the OS switches which threads have their registers in the processor at a given time.  We can talk more about this another day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how well the compiler optimizes a simple loop\n",
    "\n",
    "- **Note:** Most compilers have an options to compile with the specialized instruction set of the chip on which it is being compiled.  On pace-ice, pass `-xHost` for compiling to your current chip with `icc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $CSE6230_DIR/assignments/2-flops\n",
    "cat fma_loop_short.c | pygmentize\n",
    "module unload gcc\n",
    "module load intel/16.0\n",
    "icc -g -c -std=c99 -xHost -o fma_loop_short.o fma_loop_short.c -O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objdump -Sd fma_loop_short.o | pygmentize -l c-objdump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compiler actually compiled several version of the loop, some optimized for different inputs.\n",
    "\n",
    "There are instructions that we didnt see before, like `vfmadd213ss`.  Let's go to Intel's [intrinsics reference](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#!=undefined) to see what we can see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we learn:\n",
    "\n",
    "- There are **vectorized** instructions, when a single instruction operates on multiple data at one time (SIMD).\n",
    "- **Fused multiply add** is an instruction that counts as two flops at once!  It is so fundamental to linear algebra that it deserves optimization.\n",
    "- **Execution itself is pipelined**, with the pipeline depth depending on the instruction.\n",
    "- Sometimes there are multiple functional units that can do the same instruction (2 FPUs on a modern Intel chip, for instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Because pipelined instructions have to be independent, how many independent FMAs do we need in order to issue one per cycle to each FPU on a core, thus achieving peak flops/cycle?\n",
    "\n",
    "An application of\n",
    "\n",
    "## Little's Law\n",
    "\n",
    "$$\\Huge L = \\lambda W$$\n",
    "\n",
    "- $L$: The concurrency, number of concurrent, independent operations that will fill the pipeline\n",
    "- $\\lambda$: the \"width\" of the data that can be entered into the pipeline in a single cycle\n",
    "- $W$: the depth of the pipeline\n",
    "\n",
    "Let's do this for our setup:\n",
    "\n",
    "- $W$: the pipeline depth.  Get this from the manufacturer for your chip (if you can).  E.g. for an Intel Broadwell chip, the pipeline depth of `vfmadd*ps` instructions is 5.\n",
    "- $\\lambda$: the width of the data.  Each vector FMA operation works on 8 sets of operands, and there are 2 functional units that can execute the command, so $\\lambda = 8 * 2 = 16$.\n",
    "- Therefore I need $5 * 16 = 80$ independent FMA operations to fill the pipeline.\n",
    "- Another way to think of it: if my algorithm is composed mostly of FMAs, and it can be rewritten to be more concurrent, I expect to see a speedup up until about 80-way concurrency, and no benefit beyond that.  If my algorithm has less that 80-way concurrency, there will be bubbles in the FMA pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what is the peak flop/s per CPU core?\n",
    "\n",
    "Assuming that all floating point units (FPUs) can compute FMAs, and that each can be issued an FMA concurrently due to superscalarity,\n",
    "\n",
    "$$P_{\\text{core}} = \\#\\text{FPUs} * \\text{vector width (FMAs / FPU)} * 2 \\text{ (flops / FMA)} * \\text{throughput (1 / cycle)} * \\text{clock rate (cycles / sec)}$$\n",
    "\n",
    "So, putting in the numbers for my computer,\n",
    "\n",
    "$$P_{\\text{core}} = 2 \\text{ FPUs} * 8 \\text{ FMAs / FPU} * 2 \\text{ flops / FMA} * \\text{1 / cycle} * 3.1 \\text{ (Gigacycles / sec)} = 99.2\\text{ Gigaflop/s.}$$\n",
    "\n",
    "I have two cores, meaning $P_{\\text{total}} = 198.4$ Gigaflop/s, what we calculated in the first lecture, hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing CPU cores and GPU streaming multiprocessors (SMs)\n",
    "\n",
    "Here is a link to Prof. Vuduc's Intro to GPUs and CUDA [slides](http://vuduc.org/cse6230/slides/cse6230-fa14--05-cuda.pdf).\n",
    "\n",
    "Relevant to today's discussion:\n",
    "\n",
    "- Slides 19-24 on the execution model\n",
    "- Slides 48-51 on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key takeaways:\n",
    "\n",
    "- The CUDA programming model is Single Instruction Mutliple Thread: each thread has its own registers, but a shared instruction stream.\n",
    "- One instruction is executed on a **warp** a group of 32 threads that mostly work in lock step\n",
    "- Every instruction is vectorized, not just special instructions on the CPU.\n",
    "- Mostly: any branch divergence between them is *serialized*, so in addition to misprediction, branching has another steep price on GPUs.\n",
    "- Question: what are the depths of the pipelines on a Streaming Multiprocessor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are relevant diagrams of NVIDIA streaming multiprocessors:\n",
    "\n",
    "![SMs](./images/nvidia-kepler-vs-maxwell-sm.gif)\n",
    "\n",
    "- Every functional unit that is listed as a core handles integer and single precision floating point operations.  Double precision operations are handled by separate units, of which there are some in Kepler (1 for every 3 single precision), and few in Maxwell (1 for every 32 single precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table from NVIDIA's white paper for the Pascal architecture:\n",
    "\n",
    "![NVIDIA Table 1](./images/nvidia-table.png)\n",
    "\n",
    "Can you figure how the double and single precision flop/s are computed?\n",
    "\n",
    "- They are counting FMAs\n",
    "- Each core is a non-vectorized FPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting The Concurrency In Your Code\n",
    "\n",
    "Here is a link to Prof. Vuduc's GPU Performance Tuning [slides](http://vuduc.org/cse6230/slides/cse6230-fa14--07-gpu-tuning-1.pdf)\n",
    "\n",
    "- Relevant to today (and to your first assignment) are slides 27-40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: given all this pipelining, how can I predict the throughput of my kernel?\n",
    "\n",
    "The answer I gave in class was, that we could find the critical path through the directed acyclic graph of computations.  Something like this from [wikipedia](https://en.wikipedia.org/wiki/Directed_acyclic_graph#/media/File:Pert_chart_colored.svg), but with pipeline depth instead of months.\n",
    "\n",
    "![dag](./images/Pert_chart_colored.svg)\n",
    "\n",
    "The experts in this type of thing say that this type of analysis is easier said that done: see Section 3.2 of this recent work from [Hoffman et al.](https://arxiv.org/pdf/1702.07554.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
