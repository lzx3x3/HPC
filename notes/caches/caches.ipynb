{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 Miscellany\n",
    "\n",
    "### Due to pace-ice issues, extended to tomorrow @ 9:30.  Assignment 3 will be issued then, too.\n",
    "\n",
    "### See the clarification announcement on Canvas\n",
    "\n",
    "### One Further Note:\n",
    "\n",
    "- Do not be upset if the bandwidth that you get for the CPUs is very different from the report from Intel.\n",
    "  Unforunately, one model name does not correspond to one memory configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review: shared memory programming & shared memory hardware\n",
    "\n",
    "### In shared memory programming we are often writing our code according to a PRAM (Parallel Random Access Memory) Model.  Usually with concurrent reads, and with concurrent write policies being handled in a variety of ways:\n",
    "\n",
    "- At the algorithm level: trying to write a program where thread writes are guaranteed to be non-concurrent (partitioning the address space, coordination between threads, etc.)\n",
    "\n",
    "- At the software/runtime level: through mutexes, locks, semaphores, and so on.\n",
    "\n",
    "- At the hardware level: through atomic operations & transactional memory.\n",
    "\n",
    "### The strategies available have to take into account that *different threads have different clocks*: we cannot infer which instruction another thread is currently processing.  Strategies for concurrent writes and other synchronization between threads (such as barriers and reductions) in our algorithms must therefore avoid deadlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three critical things to understand about code *correctness* when using OpenMP for shared memory programming are:\n",
    "\n",
    "1. What is the scope of a variable: shared or private?\n",
    "\n",
    "2. What are the explicit and implicit synchronization semantics of directives (`for`, `single`, `master`, `critical`, `atomic`)?\n",
    "\n",
    "3. What program state (such as random number generator seeds) needs to persist between `parallel` regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When trying to predict the *performance* of OpenMP code, we must try to understand:\n",
    "\n",
    "1. What resources a thread is using:\n",
    "  - Which core/socket? If we don't specify *affinity* (`OMP_PROC_BIND`) it may change over the course of a program\n",
    "\n",
    "  - Which physical memory? Recall \"first touch\" placement policy, and *Non Uniform Memory Access*:\n",
    "    - If we have a streaming kernel, we will be limited by the *minimal bandwidth* in any step between core and\n",
    "      physical memory. If you are not seeing the bandwidth you expect, maybe the physical memory isn't where you\n",
    "      think it is.\n",
    "\n",
    "  - Cache (start discussing today)\n",
    "\n",
    "2. **When are threads in contention for resources?**:\n",
    "  - We've already discussed multithreading at the core: if a functional unit does not have the throughput for two\n",
    "    or more threads' operations per cycle, it will be a bottleneck.\n",
    "  \n",
    "  - Cache (start discussing today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same programming & hardware principles apply to shared memory programming on the GPU!\n",
    "\n",
    "Just a little more so, in some ways.\n",
    "\n",
    "### Scope Issues:\n",
    "\n",
    "1. Instead of only `shared` and `private` variables, CUDA has [more](https://wikis.ece.iastate.edu/cpre584/images/3/34/CUDA_Memory.pdf).  Essentially:\n",
    "  1. `__global__`, `__constant__` visible to all threads\n",
    "  2. `__shared__` visible to threads in a thread block\n",
    "  3. (unqualified) visible to just one thread\n",
    "  \n",
    "  Example coordination in computing a [reduction](https://devblogs.nvidia.com/faster-parallel-reductions-kepler/)\n",
    "\n",
    "### Thread synchronization / concurrent write issues:\n",
    "\n",
    "1. The threads within a warp are always on the same clock (as long as they don't branch diverge!)\n",
    "2. The threads in a threadblock can be synchronized, such as with the barrier `__syncthreads()`\n",
    "3. There are atomic operations for some global variable types, like `atomicAdd()`.\n",
    "4. But the scheduling order / timing of thread blocks to SMs is not directly under control,\n",
    "   so trying to coordinate between thread blocks without causing deadlocks is tricky.\n",
    "   \n",
    "### Which resources is a thread using?  When are they competing\n",
    "\n",
    "1. The streaming multiprocessor of a thread is fixed for its lifetime: no affinity required.\n",
    "2. Threads compete for space in the *register file*: register pressure leads to spilling registers to shared\n",
    "   memory, slower performance.\n",
    "3. Which physical memory?  We see in assignment 2 how big an effect this can have.\n",
    "   - Recall that we achieve peak bandwidth on the CPUs only with the benefit of *hardware prefetching* (which helps\n",
    "     us avoid bubbles in the \"load-from-memory\" pipeline)\n",
    "   - The same type of pipelined / asynchronous memory transfers are possible with the GPU, `cudaMemcpyAsync()`:\n",
    "     pipeline the memory transfer for one kernel while the other is compuing\n",
    "   - The user also can have direct control over how much shared local memory each thread block uses:\n",
    "     [see here](https://devblogs.nvidia.com/using-shared-memory-cuda-cc/).\n",
    "   - (*Folk story ahead*): this kind of explicit memory management was necessary when programming GPUs in the early      days. Thus, people had to think about memory movement, and tried to optimize for it.  They would then compare to CPU programs (which relied on automatic memory management via caches and prefetching,  etc.), where no thought was put into memory management.  100x, 1000x speed ups were reported!  Then people got serious and started comparing optimized CPU code to optimized GPU code.  The results, shockingly, looked much more similar to the hardware characteristics of devices (in terms of peak bandwidth and machine balance).\n",
    "   - Now, CUDA has implicit memory movement: \"Unified Memory\" is not only a unified address space for CPUs and GPUs, but it now anticipates your memory movement and duplicates host/device memory on device/host memory as needed.\n",
    "   - It looks like they've made ... a cache!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cache Theory through the Matrix-Matrix Multiplication Example\n",
    "\n",
    "- Our traditional serial analysis rates an algorithm by $F_f(N)$: the number of operations (flops) it takes to run for a problem of size $N$, ignoring memory movement\n",
    "\n",
    "- Because of the growing imbalance between flop/s and memop/s per second, it is not unreasonable to ignore the time it takes to do a computation and instead focus on optimizing the number of *loads* and *stores* given that we have a cache--a workspace from which we can compute--of fixed size $Z$.  Let's call this number $W_f(N,Z)$.\n",
    "\n",
    "- Just as with traditional analysis, we can develop asymptotic lower bounds, and then try to create algorithms that come close to achieving those bounds.\n",
    "\n",
    "`TODO: whiteboard Matrix Matrix multiplication example`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cache Hardware Implementations with Practical Consequences\n",
    "\n",
    "- [Hager & Wellein](https://moodle.rrze.uni-erlangen.de/pluginfile.php/12220/mod_resource/content/10/01_Arch.pdf), slides 66-96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
