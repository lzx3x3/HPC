{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage differences between MPI and OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why was OpenMP tightly integrated with C/C++ compiler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `#pragma`s that we insert require compiling something different than the code that was written (if we think of it just as C code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C\n",
    "#pragma omp parallel for\n",
    "for (i = 0; i < N; i++) {\n",
    "  loop_body(i);\n",
    "}\n",
    "```\n",
    "\n",
    "Transforms into code roughly like the following\n",
    "\n",
    "```C\n",
    "\n",
    "/* declared outside the original function */\n",
    "void omp_partial_loop_function (int start, int end, ... /* other variables from the environment used in the loop */)\n",
    "{\n",
    "  for (int i = start; i < end; i++) {\n",
    "    loop_body(i);\n",
    "  }\n",
    "}\n",
    "\n",
    "... /* back in the original function */\n",
    "\n",
    "omp_fork_threads(); /* not an actual function call, this is illustrative */\n",
    "idx = omp_get_thread_num();\n",
    "start = omp_schedule_static_start(N,i); /* not real, illustrative */\n",
    "end = omp_schedule_static_end(N,i); /* ditto */\n",
    "omp_partial_loop_function(start, end, ...);\n",
    "omp_join_threads();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why isn't MPI tightly integrated with the compiler?\n",
    "\n",
    "MPI is a C library interface with multiple implementations, and is used the same way that other C libraries are used:\n",
    "\n",
    "### 1. include the header file\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "```\n",
    "### 2. use the functions and data types in code\n",
    "\n",
    "```\n",
    "MPI_Comm comm;\n",
    "int      rank;\n",
    "int      err;\n",
    "\n",
    "err = MPI_Initialize();\n",
    "comm = MPI_COMM_WORLD;\n",
    "err = MPI_Comm_rank(comm, &rank);\n",
    "err = MPI_Finalize();\n",
    "```\n",
    "\n",
    "### 3. tell the compiler where to find the header files at compile time\n",
    "\n",
    "```\n",
    "cc -I/path/to/mpi/include -c mycode.c\n",
    "```\n",
    "\n",
    "### 4. tell the compiler where to find the libraries when at link time\n",
    "\n",
    "```\n",
    "cc -L/path/to/mpi/library -lmpi mycode.o -o myprog\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, if it isn't tightly integrated with the compiler, why is there `mpicc`?  That seems even more tightly integrated than the compiler.\n",
    "\n",
    "`mpicc` is essentially a wrapper around a compiler that automatically does steps 3 and 4 for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does MPI have a launcher and OpenMP doesn't?\n",
    "\n",
    "MPI code is written from the perspective of a single thread of computation working independently, which is actually a separate _process_ from the OS perspective, whereas OpenMP is mostly written from the perspective of the master thread delegating to forked threads.  `mpirun` and `mpiexec` help the OS set up the independent processes, and does some of the things that are handled by the environment variables for OpenMP (e.g. `OMP_NUM_THREADS=8` becomes `mpiexec -n 8` or `mpirun -np 8`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isn't it more overhead to have 8 independent OS processes than to have 1 OS processes with 8 threads?\n",
    "\n",
    "Not as much as you'd think: in linux they are handled by different uses of the same system call `clone` with different semantics about how memory is passed from the cloner to the clonee:\n",
    "\n",
    "- With threads, the clonee sees and shares the memory addresses of the cloner (which is why variables in OpenMP declared outside of a parallel scope are **shared by default**)\n",
    "- With processes, the clonee gets a copy of the cloner's memory, but it is implemented by **copy-on-write**.  This means that if there is a large portion of read-only memory (like the code from libraries that the program links to), it doesn't actually get duplicated in physical memory, it just gets _new virtual memory addresses_ to the _same physical memory_.  We will see in more advanced MPI that MPI processes can in fact share writeable memory, but you have to ask for it: MPI memory is **private by default**\n",
    "- Thus the one real overhead of processes over threads is on the part of the memory hierarchy of a chip that translates virtual addresses into physical addresses.  If that is not the bottleneck, then processes will be as efficient as threads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
