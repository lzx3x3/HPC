{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI: Complexity of collectives and almost collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall: Important differences between different point-to-point MPI routines\n",
    "\n",
    "### Blocking semantics: whether the operands and results are safe to use when the function returns:\n",
    "\n",
    "- `MPI_Send()` / `MPI_Recv()`: safe to use\n",
    "\n",
    "- `MPI_Isend()` / `MPI_Irecv()`: wait until `MPI_Request` is complete (`MPI_Wait()` or do other things with `MPI_Test()`)\n",
    "\n",
    "### Synchronization semantics: has the matching call initiated?\n",
    "\n",
    "- `MPI_Ssend()` / `MPI_Issend()`: yes!\n",
    "- `MPI_Send()` / `MPI_Isend()`: not necessarily!\n",
    "- Clearly when `MPI_Recv()`/`MPI_Irecv()` results are safe to use the matching `MPI_Xsend()`  has initiated :)\n",
    "\n",
    "### Protocol: how is the message sent?\n",
    "\n",
    "- **Rendezvous**: only the *envelope* (data type, data size, source, tag, communicator) is send immediately; the destination replies when it is ready\n",
    "  - Bad for latency (multiple trips in the network), good for memory movement (no additional copies)\n",
    "- **Eager**: the whole message is sent immediately, stored in an MPI buffer, copied into the destination buffer when the matching `MPI_Recv()` is called\n",
    "  - Good for latency, but buffer space and memory movement become harder to accomodate with larger message sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One aspect not discussed last time: ordering, progress, and fairness\n",
    "\n",
    "- **ordering** If `A` sends two messages to `B`, will they be received in that order? Yes!\n",
    "\n",
    "- **progress** If an `MPI_Send()` on `A` matches an `MPI_Recv()` on `B`, at least one of them will complete:\n",
    "  - The `MPI_Recv()` could match another message with `MPI_ANY_SOURCE`, so the `MPI_Send()` may not complete always\n",
    "  - (In multithreading) a different thread at the same process could match the `MPI_Send()`, so the `MPI_Recv()` may not complete always\n",
    "  \n",
    "- **fairness** No guarantee! see [this advice](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node58.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall: Collective operations on a communicator\n",
    "\n",
    "### See the distributed array charts from [Prof. Vuduc's notes](http://vuduc.org/cse6230/slides/cse6230-fa14--06-mpi.pdf)\n",
    "\n",
    "- `MPI_Barrier()`\n",
    "- `MPI_Reduce()` $\\leftrightarrow$ `MPI_Bcast()`\n",
    "- `MPI_Gather()` $\\leftrightarrow$ `MPI_Scatter()`\n",
    "  - Varying sizes, `MPI_Gatherv()` $\\leftrightarrow$ `MPI_Scatterv()`\n",
    "- `MPI_Allreduce()`\n",
    "- `MPI_Allgather()`\n",
    "  - Varying sizes `MPI_Allgatherv()`\n",
    "- `MPI_Alltoall()`\n",
    "  - Varying sizes `MPI_Alltoallv()`\n",
    "  \n",
    "### Same questions as with point-to-point: what are the blocking, synchronization, and protocols?\n",
    "\n",
    "- All of the above are **blocking**.\n",
    "- Are there non-blocking versions?\n",
    "  - As of MPI-2, yes! `MPI_Ibarrier()`, etc.\n",
    "  - Are they useful? Yes, but we have to be careful about order and progress.\n",
    "  \n",
    "- `MPI_Barrier()` and the `MPI_All`s are inherently **synchronous**; `MPI_Reduce()`, `MPI_Bcast()`, `MPI_Scatter()`, and `MPI_Gather()` are **not**.\n",
    "\n",
    "- Like for point-to-point, the **protocols** are system, implementation, and message content dependent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing the real performance of a distributed memory system\n",
    "\n",
    "Network topology is going to be important for good predictions, but for now we can pretend we are using a totally connected network.\n",
    "\n",
    "### Last time I mentioned the **LogP** model:\n",
    "\n",
    "- **L**: the latency between a one byte message being sent and received\n",
    "- **o**: the process overhead of initiating a `send()`/`recv()`\n",
    "- **g**: the inverse bandwidth (traditionally, applied to all bytes after the first)\n",
    "- **P**: the number of nodes in the network\n",
    "\n",
    "### For an even simple analysis, we can\n",
    "\n",
    "- combine $L$ and $o$ into the $\\lambda$, the latency of a message\n",
    "- apply **g** to all bytes of a message\n",
    "- look at the *critical path*: the $R$ dependent messages, communicating total of $W$ words, that maximizes\n",
    "$\\lambda R + gW$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are real world estimates of $\\lambda$ and $g$?\n",
    "\n",
    "- [Hager & Wellein, slide 66](https://moodle.rrze.uni-erlangen.de/pluginfile.php/12220/mod_resource/content/10/01_Arch.pdf)\n",
    "\n",
    "- Using the values from that slide $g$ is four orders of magnitude smaller that $\\lambda$\n",
    "- Recent networks haven't improved that ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whiteboard exercises: estimating MPI_Collective performance in $\\lambda$, $g$, and $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `MPI_Reduce()` an object of size $b$ bytes: $O((\\lambda + gb)\\log_2 P)$\n",
    "  - What if we could send and receive $k$ messages simultaneously?\n",
    "- `MPI_Bcast()`: the same!\n",
    "- `MPI_Barrier()`: ?\n",
    "- `MPI_Gather()`: $O(\\lambda \\log P + gb P)$\n",
    "- `MPI_Scatter()`: the same!\n",
    "- `MPI_Allreduce()`: Can we bound in terms of the ones we've already done?\n",
    "- `MPI_Allgather()`: ?\n",
    "- `MPI_Alltoall()`: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of taking machine parameters into account: CG vs. Pipelined CG\n",
    "\n",
    "- [Ghysels and Vanroose, 2013](http://dx.doi.org/10.1016/j.parco.2013.06.001)\n",
    "- Fusing messages together: no advantage in $g$, but less $\\lambda$ (which is much larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of taking machine parameters and message characteristics into account: Many-to-many\n",
    "\n",
    "- Process $p$ want to send $k_p$ messages of varying sizes\n",
    "- No process knows _a priori_ where it will receive messages from\n",
    "  - Suppose that unknown number for $p$ is $r_p$: what if we don't know it exactly, but we can bound it?\n",
    "    $$K = \\max_p \\max (k_p, r_p)$$\n",
    "    \n",
    "  - How does $K$ affect our choice of algorithm?\n",
    "  - If $K \\sim P$ (dense) we should build our approach out of collective operations\n",
    "  - If $K < \\log P$ (sparse), we should try to use point-to-point operations where possible\n",
    "  - Regardless, we can guarantee an $O(\\log P)$ lower bound: why?\n",
    "\n",
    "### Whiteboard: what are some solutions?\n",
    "\n",
    "- Global exchange\n",
    "- Global census\n",
    "  - `MPI_Reduce_scatter` and `MPI_Reduce_scatter_block`!\n",
    "- Static routing, randomized routing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
