{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI & Distributed Memory Programming Models \n",
    "\n",
    "![net](./images/net.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications/documentation:\n",
    "\n",
    "\n",
    "- [http://mpi-forum.org/docs/](http://mpi-forum.org/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are many implementations:\n",
    "\n",
    "- [MPICH](http://www.mpich.org/)\n",
    "- [MVAPICH](http://mvapich.cse.ohio-state.edu/)\n",
    "- [Open-MPI](https://www.open-mpi.org/)\n",
    "- [Intel MPI](https://software.intel.com/en-us/intel-mpi-library/)\n",
    "- [IBM Spectrum / BG/Q](https://www.ibm.com/us-en/marketplace/spectrum-mpi)\n",
    "- Other vendors...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "\n",
    "- [From Lawrence Livermore National Laboratory](https://computing.llnl.gov/tutorials/mpi/)\n",
    "- [mpitutorial.com (with examples on github)](http://mpitutorial.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI Structure\n",
    "\n",
    "- MPI is the Message Passing Interface:\n",
    "    - a library of C routines (`MPI_Init()`, `MPI_Finalize()`)\n",
    "    - that allow a program to interact with the MPI runtime process manager\n",
    "    - which is started with a launcher (`mpiexec`, `mpirun`, cluser-specific)\n",
    "    - and *implementation specific* environment variables\n",
    "\n",
    "---\n",
    "\n",
    "- Compared to OpenMP:\n",
    "    - No `#pragma`s\n",
    "        - an MPI program is an MPI program: there is no \"turning off\" MPI\n",
    "        - though you can/should be able to run with one process\n",
    "        - there are \"dummy\" implementations of MPI that only allow one process (`mpiuni` that ships with PETSc)\n",
    "        - no need for \"compiler support\" for MPI: should work with any valid compiler\n",
    "    - Environment variables are implementation specific, not part of the standard\n",
    "        - Typically a last-step optimization rather than a primary control method\n",
    "        - Affinity, choosing particular algorithms, buffer sizes, etc.\n",
    "    - The launcher (`mpiexec`/`mpirun`)\n",
    "        - A program called without the launcher is like a `#pragma omp for`\n",
    "          outside of `#pragma omp parallel`: valid, but serial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `helloworld.c`\n",
    "\n",
    "- MPI can be treated like any other library:\n",
    "    - `#include <mpi.h>` header file, `-I/path/to/mpi/include` when compiling\n",
    "    - `-L/path/to/mpi/lib -lmpi` when linking\n",
    "\n",
    "- But MPI has a convenience wrapper for the compiler `mpicc`, `mpic++`\n",
    "\n",
    "---\n",
    "\n",
    "- In general, `MPI_Init()` should be the first thing you do in an MPI program,\n",
    "  `MPI_Finalize()` should be the last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m#\u001b[39;49;00m\u001b[36minclude\u001b[39;49;00m \u001b[37m<stdio.h>\u001b[39;49;00m\u001b[36m\u001b[39;49;00m\n",
      "\u001b[36m#\u001b[39;49;00m\u001b[36minclude\u001b[39;49;00m \u001b[37m<mpi.h>\u001b[39;49;00m\u001b[36m\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mint\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m argc, \u001b[36mchar\u001b[39;49;00m **argv)\n",
      "{\n",
      "  \u001b[36mint\u001b[39;49;00m ierr;\n",
      "\n",
      "  ierr = MPI_Init (&argc, &argv);\n",
      "  \u001b[34mif\u001b[39;49;00m (ierr) \u001b[34mreturn\u001b[39;49;00m ierr;\n",
      "\n",
      "  printf(\u001b[33m\"\u001b[39;49;00m\u001b[33mHello, world!\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m);\n",
      "\n",
      "  ierr = MPI_Finalize();\n",
      "  \u001b[34mreturn\u001b[39;49;00m ierr;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pygmentize helloworld.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-to-Point Communication\n",
    "\n",
    "- The [LLNL tutorial](https://computing.llnl.gov/tutorials/mpi/#Routine_Arguments) has manual pages for the routines we talk about today:\n",
    "\n",
    "---\n",
    "\n",
    "- Blocking: routines that only return when read/write buffers are safe to read-from/write-to:\n",
    "    - [`MPI_Send`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Send.txt):\n",
    "      implementation chooses when/if this routine can cause **deadlocks** by waiting to return for a matching recv that never posts.\n",
    "    - [`MPI_Ssend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Ssend.txt):\n",
    "      waits until a matching recv has posted, thus can cause deadlocks.\n",
    "    - [`MPI_Bsend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Bsend.txt):\n",
    "      buffers message before sending, so behavior is always *local* (does not depend on destination behavior).\n",
    "      See [`pointtopoint-8.c`](pointtopoint/pointtopoint-8.c) about setting up the buffers properly.\n",
    "    - [`MPI_Rsend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Rsend.txt):\n",
    "      Only valid if a matching recv is ready at the destination. **Note**: rarely used, and so it seems not implemented properly.\n",
    "      [`pointtopoint-9.c`](pointtopoint/pointtopoint-9.c) should fail, but MPICH treats this like `MPI_Ssend`.\n",
    "\n",
    "---\n",
    "\n",
    "- Blocking: routines that only return when read/write buffers are safe to read-from/write-to:\n",
    "    - [`MPI_Recv`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Recv.txt)\n",
    "    - [`MPI_Probe`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Probe.txt):\n",
    "      useful when you have a desired *envelope* (source, tag,\n",
    "      communicator), but do not know the data.\n",
    "    - [`MPI_Get_count`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_count.txt):\n",
    "      Combine with `MPI_Probe` to allocate buffers once the message size is known (see\n",
    "      [`pointtopoint-3.c`](pointtopoint/pointtopoint-3.c))\n",
    "    - [`MPI_Mprobe`](http://www.mpich.org/static/docs/latest/www3/MPI_Mprobe.html):\n",
    "      thread safe probe that gives a handle to a *message*, which can then be received with\n",
    "    - [`MPI_Mrecv`](http://www.mpich.org/static/docs/latest/www3/MPI_Mrecv.html).\n",
    "\n",
    "---\n",
    "\n",
    "- Non-blocking: routines return immediately, and create an `MPI_Request`.  Buffers/results are\n",
    "  only safe when the request is completed. \n",
    "    - [`MPI_Isend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Isend.txt)\n",
    "    - [`MPI_Issend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Issend.txt)\n",
    "    - [`MPI_Ibsend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Ibsend.txt)\n",
    "    - [`MPI_Irsend`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Irsend.txt)\n",
    "    - [`MPI_Irecv`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Irecv.txt)\n",
    "    - [`MPI_Iprobe`](https://computing.llnl.gov/tutorials/mpi/man/MPI_probe.txt)\n",
    "    - [`MPI_Improbe`](http://www.mpich.org/static/docs/latest/www3/MPI_Improbe.html)\n",
    "    - [`MPI_Imrecv`](http://www.mpich.org/static/docs/latest/www3/MPI_Imrecv.html)\n",
    "\n",
    "---\n",
    "\n",
    "- Handling the requests generated by non-blocking functions:\n",
    "    - [`MPI_Wait`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Wait.txt):\n",
    "      wait until the communication initiated by the non-blocking function\n",
    "      reaches the same state as the non-blocking variant.  In other words,\n",
    "      calling `MPI_Ixyz(...,&req); MPI_Wait(&req,&status);` should have the\n",
    "      same blocking behavior as `MPI_Xyz(...,&status);`.\n",
    "    - [`MPI_Test`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Test.txt):\n",
    "      sets a Boolean true/false flag when the communication initiated by the\n",
    "      non-blocking function reaches the same state as the non-blocking variant.\n",
    "      In other words, calling `MPI_Ixyz(...,&req); MPI_Test(&req,&flag,&status);`\n",
    "      will set flag to `1` only if `MPI_Xyz(...,&status);` would have completed.\n",
    "\n",
    "---\n",
    "\n",
    "- Handling the requests generated by non-blocking functions:\n",
    "    - [`MPI_Waitall`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitall.txt):\n",
    "      Wait until all requests in an array have completed.\n",
    "    - [`MPI_Testall`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Testall.txt):\n",
    "      True only if all requests in an array have completed.\n",
    "    - [`MPI_Waitany`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitany.txt):\n",
    "      Wait until at least one request in an array has completed, indicates which one it was.\n",
    "    - [`MPI_Testany`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Testany.txt):\n",
    "      True only if at least one request in an array has completed, indicates which one it was.\n",
    "    - [`MPI_Waitsome`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitsome.txt):\n",
    "      Wait until at least some requests in an array have completed, indicates which ones they were.\n",
    "    - [`MPI_Testsome`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Testsome.txt):\n",
    "      True only if at least some requests in an array has completed, indicates which ones they were.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication Protocols\n",
    "\n",
    "- [William Gropp's Tutorial at Argonne](https://www.mcs.anl.gov/research/projects/mpi/tutorial/perf/mpiperf/index.htm) has figures for protocols like we saw in class:\n",
    "    - [Eager](https://www.mcs.anl.gov/research/projects/mpi/tutorial/perf/mpiperf/sld019.htm)\n",
    "    - [Rendezvous](https://www.mcs.anl.gov/research/projects/mpi/tutorial/perf/mpiperf/sld022.htm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "- [Professor Chow's Slides on MPI](https://www.cc.gatech.edu/~echow/ipcc/hpc-course/14_mpi.pdf) (slide 13)\n",
    "  show time to send a message based on its length:\n",
    "    - Short message time dominated by network *latency* `L`\n",
    "    - Long message time dominated by reciprocal of network *bandwidth* `G = 1/B`\n",
    "    - **Question in class**: why does the switch from eager to rendezvous protocol make it *faster*?\n",
    "        - Answer: eager requires copying from MPI buffer to user buffer; rendezvous requires no copies.\n",
    "          For large enough messages, `memcpy` takes longer than the network handshake (~`2L`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Models\n",
    "\n",
    "- When a program can be structured into \"bulk phases\" of alternating computation and communication, we can model message passing with the [bulk synchronous parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel) model which is related to the model at the start of [Prof. Vuduc's MPI notes](http://vuduc.org/cse6230/slides/cse6230-fa14--06-mpi.pdf).  The key machine parameters are:\n",
    "    1. The number of processes/nodes `P`\n",
    "    2. The maximum number of messages that can be sent/received by one process at a time `k` simultaneously (if not specified, assume `k`=1)\n",
    "    3. The latency `L` of a message transmission\n",
    "    4. The bandwidth `B` of a message transmission\n",
    "    5. The overhead of synchronization (if it is an explicitly synchronized model)\n",
    "    \n",
    "The time of transmission of a single message of length `b` is (`L + b / B` = `L + (1/B) * b`).  It is also common to see these network parameters are $L = \\lambda$ and $1/B = \\mu$,\n",
    "so that the message time is $(\\lambda + \\mu b)$\n",
    "\n",
    "\n",
    "- If the communication is less structured, we need a model that allows for messages to be sent at any time.  A good start is [LogGP (Alexandrov et al. 97)](https://people.eecs.berkeley.edu/~mme/cs267-2016/papers/loggp.pdf):\n",
    "  - `L`: Network latency\n",
    "  - `o`: overhead of initiating send / completing recv\n",
    "  - `g`: the \"gap\" between inserting consecutive messages into the network\n",
    "  - `G`: the \"Gap per byte\": the reciprocal of network bandwidth\n",
    "  - `P`: the number of processes\n",
    "  - Not present: an idea of network \"congestion\" from multiple concurrent sends/recvs\n",
    "\n",
    "\n",
    "### Question: How would we measure these machine parameters using MPI?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What makes measuring difficult?**\n",
    "\n",
    "- Every process is potentially on a different clock, and reaches code at different times.\n",
    "- The return time of a function on one process does not include the entire network time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point-to-point timing: Ping Pong**  A ping-pong test involves one message being repeatedly sent back and forth between two processes.  Because one ping-pong originates and ends on the same process, the timing on a single process includes all of the network time.\n",
    "\n",
    "### A suite of benchmarks:\n",
    "\n",
    "[OSU Micro-benchmarks](http://mvapich.cse.ohio-state.edu/benchmarks/)\n",
    "\n",
    "Includes ping-pong tests, other point-to-point tests (e.g. bidirectional bandwidth), collective tests, tests including memory transfer to GPUs, etc.\n",
    "\n",
    "**We look at a modified version of the OSU point-to-point ping-pong test in class** the source is included in the [osu_modified](./osu_modified) directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Communication\n",
    "\n",
    "- Like point-to-point routines, the collective communication routines have\n",
    "  blocking and non-blocking variants.\n",
    "\n",
    "---\n",
    "\n",
    "- Blocking:\n",
    "\n",
    "    - [`MPI_Barrier`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Barrier.txt):\n",
    "      block program progress until all processes have reached the same barrier.\n",
    "        - When processes are running on a shared-memory socket, this can be\n",
    "          implemented efficiently: it can execute as quickly as a memory\n",
    "          location that receives multiple writes can be updated in the cache\n",
    "          for each process.  So the best-case latency of a barrier would be the\n",
    "          cache-coherence latency of the socket.\n",
    "        - For an abstract machine model like `LogGP` that has point-to-point primitives,\n",
    "          Barrier can be implemented in a way that takes `O(log P)`.\n",
    "        - Some machines like [IBM Blue Gene/Q](https://www.alcf.anl.gov/files/IBM_BGQ_Architecture_0.pdf)\n",
    "          Have special networks dedicated to collective operations like barrier\n",
    "\n",
    "---\n",
    "\n",
    "- [`MPI_Bcast`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Bcast.txt):\n",
    "  Copy a value (or vector of values) from one process to every process of the communicator.\n",
    "- [`MPI_Scatter`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Scatter.txt):\n",
    "  Scatter many values from one process to every processes of the communicator.\n",
    "- [`MPI_Reduce`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce.txt):\n",
    "  Combine values from every process into a single value on one process using a reduction operation.\n",
    "    - `MPI_SUM`\n",
    "    - `MPI_MIN`\n",
    "    - `MPI_MAX`\n",
    "    - `MPI_PROD`\n",
    "    - `MPI_LAND`: logical and\n",
    "    - `MPI_BAND`: bitwise and\n",
    "    - `MPI_LOR`: logical or\n",
    "    - `MPI_BOR`: bitwise or\n",
    "    - `MPI_LXOR`: logical exclusive or\n",
    "    - `MPI_BXOR`: bitwise exclusive or\n",
    "    - `MPI_MINLOC`: operates on `(value, rank)` pairs, returns the minimum and the rank from which the minimum originated\n",
    "    - `MPI_MAXLOC`: operates on `(value, rank)` pairs, returns the maximum and the rank from which the maximum originated\n",
    "    - `MPI_REPLACE`: returns the second input `replace(a,b) := b`\n",
    "\n",
    "---\n",
    "\n",
    "- [`MPI_Gather`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Gather.txt):\n",
    "  Gather many values from every processes of the communicator to one process.\n",
    "- [`MPI_Allreduce`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Allreduce.txt):\n",
    "  Combine values from every process into a single value on every process using a reduction operation.\n",
    "- [`MPI_Allgather`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Allgather.txt):\n",
    "  Gather values from every process to every process.\n",
    "- [`MPI_Alltoall`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Alltoall.txt):\n",
    "  Every process send a unique value to every other process.\n",
    "\n",
    "---\n",
    "\n",
    "- Nonblocking (a new feature in MPI-3):\n",
    "    - [`MPI_Ibarrier`](http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html)\n",
    "    - [`MPI_Ibcast`](http://www.mpich.org/static/docs/v3.1/www3/MPI_Ibcast.html)\n",
    "    - [`MPI_Iscatter`](http://www.mpich.org/static/docs/latest/www3/MPI_Iscatter.html)\n",
    "    - [`MPI_Ireduce`](http://www.mpich.org/static/docs/latest/www3/MPI_Ireduce.html)\n",
    "    - [`MPI_Igather`](http://www.mpich.org/static/docs/latest/www3/MPI_Igather.html)\n",
    "    - [`MPI_Iallreduce`](http://www.mpich.org/static/docs/latest/www3/MPI_Iallreduce.html)\n",
    "    - [`MPI_Iallgather`](http://www.mpich.org/static/docs/latest/www3/MPI_Iallgather.html)\n",
    "    - [`MPI_Ialltoall`](http://www.mpich.org/static/docs/latest/www3/MPI_Ialltoall.html)\n",
    "\n",
    "---\n",
    "\n",
    "- Variable sizes: scatter, gather and all-to-all routines have variants for a\n",
    "  variable message size per process:\n",
    "    - [`MPI_Scatterv`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Scatterv.txt)\n",
    "    - [`MPI_Gatherv`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Gatherv.txt)\n",
    "    - [`MPI_Allgatherv`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Allgatherv.txt)\n",
    "    - [`MPI_Alltoallv`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Alltoallv.txt)\n",
    "    - [`MPI_Iscatterv`](http://www.mpich.org/static/docs/latest/www3/MPI_Iscatterv.html)\n",
    "    - [`MPI_Igatherv`](http://www.mpich.org/static/docs/latest/www3/MPI_Igatherv.html)\n",
    "    - [`MPI_Iallgatherv`](http://www.mpich.org/static/docs/latest/www3/MPI_Iallgatherv.html)\n",
    "    - [`MPI_Ialltoallv`](http://www.mpich.org/static/docs/latest/www3/MPI_Ialltoallv.html)\n",
    "\n",
    "---\n",
    "\n",
    "- Scans: [Prefix sum](https://en.wikipedia.org/wiki/Prefix_sum) operations over\n",
    "  values distributed between proceses\n",
    "    - [`MPI_Scan`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Scan.txt):\n",
    "      the reduction over myself and all lesser ranked processes of values stored by each\n",
    "    - [`MPI_Exscan`](https://computing.llnl.gov/tutorials/mpi/man/MPI_Exscan.txt):\n",
    "      the reduction over all lesser ranked processes of values stored by each\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
