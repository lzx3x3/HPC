{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cores & Memory: Streaming Kernels\n",
    "\n",
    "![streaming memory](./images/ram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## - Assignment 1 Review\n",
    "## - Review: Little's Law, CPUs vs. GPUs\n",
    "## - Streaming Kernels\n",
    "## - Arithmetic Intensity, Machine Balance, & the Roofline Model\n",
    "\n",
    "Each of these three concepts is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Questions / confusion people have had:\n",
    "\n",
    "#### - \"Intel says this is a 6-core / 12-hardware thread package; PACE says this is a 12 core node.  Is PACE wrong?\"\n",
    "\n",
    "Try this on a compute node when you have X forwarding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine (256GB total)\n",
      "  NUMANode L#0 (P#0 128GB)\n",
      "    Socket L#0 + L3 L#0 (10MB)\n",
      "      L2 L#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n",
      "      L2 L#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#1)\n",
      "      L2 L#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#2)\n",
      "      L2 L#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#3)\n",
      "    HostBridge L#0\n",
      "      PCIBridge\n",
      "        PCI 15b3:1003\n",
      "          Net L#0 \"ib0\"\n",
      "          OpenFabrics L#1 \"mlx4_0\"\n",
      "      PCIBridge\n",
      "        PCI 10de:15f8\n",
      "      PCIBridge\n",
      "        PCIBridge\n",
      "          PCI 1a03:2000\n",
      "      PCIBridge\n",
      "        PCI 8086:1521\n",
      "          Net L#2 \"eth0\"\n",
      "        PCI 8086:1521\n",
      "          Net L#3 \"eth1\"\n",
      "      PCI 8086:8d02\n",
      "        Block L#4 \"sda\"\n",
      "  NUMANode L#1 (P#1 128GB)\n",
      "    Socket L#1 + L3 L#1 (10MB)\n",
      "      L2 L#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#4)\n",
      "      L2 L#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#5)\n",
      "      L2 L#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#6)\n",
      "      L2 L#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#7)\n",
      "    HostBridge L#6\n",
      "      PCIBridge\n",
      "        PCI 10de:15f8\n"
     ]
    }
   ],
   "source": [
    "module load hwloc\n",
    "lstopo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that PACE has it right because, for all of the nodes we will be using, they have installed *two sockets per node*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those seeking peak GPU performance, you can also control the \"grid size\" (number of thread blocks)\n",
    "\n",
    "Add, e.g., `Gs=15` to the `run_fma_prof` and `run_fma_prof_opt` targets\n",
    "\n",
    "This will let you experiment with ILP vs. TLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Little's Law\n",
    "\n",
    "$\\Huge L = \\lambda W$\n",
    "\n",
    "#### - $L$: amount of $x$ in a system.\n",
    "#### - $\\lambda$: arrival rate, $x$ / sec.\n",
    "#### - $W$: time spent in the system (sec).\n",
    "\n",
    "- Note that it has dimensions $x$, whatever it is we're trying to count.\n",
    "- For pipelines, we often think of $\\lambda$ as $x$ / cycle and $W$ as length of the pipeline in cycles.  Every unit but $x$ cancels out, so we can take whichever form is more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example from last time: how many independent FMAs are needed for peak flop/s?\n",
    "\n",
    "#### - $W$: depth of the pipeline\n",
    "#### - $\\lambda$: arrival rate = # FPUs * # FMAs in a *vectorized* instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Intel's intrisics guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/)]\n",
    "\n",
    "![vfmadd132ps](./images/intel-intrinsics.png)\n",
    "\n",
    "latency = $W$, CPI (cycles per instruction) = 1 / $\\lambda_v$, where $\\lambda_v$ = throughput for *vectorized* FMAs.  Multiply by vector width (see operation pseudocode) to get full $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Question from last time:\n",
    "\n",
    "#### - What is the latency of FMA on the GPUs?  How could we estimate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Review: CPUs  (Hosts)\n",
    "\n",
    "#### - One set of instructions per thread\n",
    "#### - OS schedules threads (software multithreading), can *migrate* them between cores\n",
    "#### - x86-64 (AVX2) instruction set has 32 256-bit vector registers per threads\n",
    "#### - Parallelism via software multithread, hardware multithreading, superscalar execution, vectorization (**SIMD**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Review: GPUs (Devices)\n",
    "\n",
    "(See the nice illustrations from Prof. Vuduc's [slides](http://vuduc.org/cse6230/slides/cse6230-fa14--05-cuda.pdf), starting on slide 27)\n",
    "\n",
    "#### - A *compute kernel* is a task that the host assigns to the device in a kernel launch\n",
    "\n",
    "```c++\n",
    "\n",
    "solveForX<<<ThreadsPerBlock,BlocksPerGrid>>>(A,x,y);\n",
    "```\n",
    "\n",
    "- Proceeds asynchronously from the host until the host requires the results\n",
    "\n",
    "#### - The task is broken down into a **grid** of *independent* thread blocks\n",
    "\n",
    "- The host has no control over which thread blocks are assigned where and in what order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### - Each thread block is assigned to a streaming multiprocessor (SM), where it stays\n",
    "\n",
    "- A SM may be assigned multiple thread blocks\n",
    "\n",
    "#### - The SM breaks down the thread blocks into **warps** (32 threads): a warp shares an instruction set, so all (non-divergent) instructions are vectorized\n",
    "#### - The SM issues instructions from multiple warps per cycle (sometimes multiple instructions per warp per cycle) \n",
    "#### - When a warp is stalled, another is scheduled\n",
    "#### - Avoid: thread divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using `nvprof` to estimate the FMA latency\n",
    "\n",
    "`nvprof` is a performance analysis tool like `perf` and `gprof` combined for the GPU\n",
    "\n",
    "It can be inserted before a program in the same way as `perf`.  Here's the standard invocation of our\n",
    "program from the first assignment.  First info about what kind of GPU I'm using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 29 09:44:55 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   23C    P0    25W / 250W |      0MiB / 16280MiB |      0%   E. Process |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   23C    P0    25W / 250W |      0MiB / 16280MiB |      0%   E. Process |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o *.optrpt *.so fma_prof fma_prof_opt\n",
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_prof.o fma_prof.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_omp.o fma_omp.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_loop_host.o fma_loop_host.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "nvcc -ccbin=icpc -Xcompiler '-fPIC'  -dc -o fma_cuda.o fma_cuda.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -Xcompiler '-fPIC'  -dc -o fma_loop_dev.o fma_loop_dev.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -Xcompiler '-fPIC' -dlink  fma_cuda.o fma_loop_dev.o -o fma_cuda_link.o\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "icpc -qopenmp -shared -Wl,-soname,libfma_cuda.so -o libfma_cuda.so fma_cuda_link.o fma_cuda.o fma_loop_dev.o -L/usr/local/pacerepov1/cuda/8.0.44/lib64 -Wl,-rpath,/usr/local/pacerepov1/cuda/8.0.44/lib64 -lcudart\n",
      "icpc -qopenmp -o fma_prof fma_prof.o fma_omp.o fma_loop_host.o libfma_cuda.so -Wl,-rpath,.\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1  ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "[./fma_prof]: 1.392913e-02 elapsed seconds\n",
      "[./fma_prof]: 114688000000 flops executed\n",
      "[./fma_prof]: 8.233681e+12 flop/s\n"
     ]
    }
   ],
   "source": [
    "cd $CSE6230_DIR/assignments/2-flops\n",
    "git checkout fma_loop_dev.cu\n",
    "make clean\n",
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we are using the maximum thread block size (1024) and making two times as many thread blocks as there are SMs (56), leading to 2048 threads per SM, the maximum **occupancy**.\n",
    "\n",
    "#### - **Occupancy**: the ratio of active warps in an SM (GPU) to the maximum number of warps per SM (GPU). \n",
    "\n",
    "A good measure of whether there is additional *thread-level parallelism* on the device.  Let's see if we can measure that.\n",
    "\n",
    "(Note: for this GPU, I could get closer to 100% occupancy with a block size of 512 and twice as many blocks.  Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              achieved_occupancy:  Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor\n",
      "              achieved_occupancy:  Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor\n"
     ]
    }
   ],
   "source": [
    "nvprof --query-metrics | grep occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1 nvprof --metrics achieved_occupancy ./fma_prof 0 114688 512 224 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 512\n",
      "==22377== NVPROF is profiling process 22377, command: ./fma_prof 0 114688 512 224 250000 0.5 3.0\n",
      "[./fma_prof]: 5.339694e-02 elapsed seconds\n",
      "[./fma_prof]: 114688000000 flops executed\n",
      "[./fma_prof]: 2.147838e+12 flop/s\n",
      "==22377== Profiling application: ./fma_prof 0 114688 512 224 250000 0.5 3.0\n",
      "==22377== Profiling result:\n",
      "==22377== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.976454    0.976454    0.976454\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.818821    0.818821    0.818821\n",
      "Device \"Tesla P100-PCIE-16GB (1)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.976738    0.976738    0.976738\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.815059    0.815059    0.815059\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=512 Gs=$((56*4)) Nd=$((1024*56*2)) T=250000 PERF=\"nvprof --metrics achieved_occupancy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Okay, but let's look at the measure that we're actually interested, the percentag of peak flop/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              flop_hp_efficiency:  Ratio of achieved to peak half-precision floating-point operations\n",
      "              flop_sp_efficiency:  Ratio of achieved to peak single-precision floating-point operations\n",
      "              flop_dp_efficiency:  Ratio of achieved to peak double-precision floating-point operations\n",
      "              flop_hp_efficiency:  Ratio of achieved to peak half-precision floating-point operations\n",
      "              flop_sp_efficiency:  Ratio of achieved to peak single-precision floating-point operations\n",
      "              flop_dp_efficiency:  Ratio of achieved to peak double-precision floating-point operations\n"
     ]
    }
   ],
   "source": [
    "nvprof --query-metrics | grep flop | grep peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1 nvprof --metrics flop_sp_efficiency ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "==23105== NVPROF is profiling process 23105, command: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==23105== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.\n",
      "==23105== Replaying kernel \"__nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==23105== Some kernel(s) will be replayed on device 1 in order to collect all events/metrics.\n",
      "==23105== Replaying kernel \"__nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==23105== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "==23105== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "[./fma_prof]: 3.038997e+00 elapsed seconds\n",
      "[./fma_prof]: 114688000000 flops executed\n",
      "[./fma_prof]: 3.773877e+10 flop/s\n",
      "==23105== Profiling application: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==23105== Profiling result:\n",
      "==23105== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      48.41%      48.41%      48.41%\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "Device \"Tesla P100-PCIE-16GB (1)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      48.20%      48.20%      48.20%\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000 PERF=\"nvprof --metrics flop_sp_efficiency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So while we have good occupancy, we are acheiving only ~50% of peak flop/s.\n",
    "\n",
    "First, let's unroll things just a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/assignments/2-flops/fma_loop_dev.cu b/assignments/2-flops/fma_loop_dev.cu\n",
      "index c603504..d556288 100644\n",
      "--- a/assignments/2-flops/fma_loop_dev.cu\n",
      "+++ b/assignments/2-flops/fma_loop_dev.cu\n",
      "@@ -9,6 +9,7 @@ fma_loop_dev (int N, int T, float *a, float b, float c)\n",
      "   int num_threads = gridDim.x * blockDim.x;\n",
      " \n",
      "   for (int i = my_thread; i < N; i+= num_threads) {\n",
      "+#pragma unroll 64\n",
      "     for (int j = 0; j < T; j++) {\n",
      "       a[i] = a[i] * b + c;\n",
      "     }\n"
     ]
    }
   ],
   "source": [
    "sed -i -e '/for (int j/i #pragma unroll 64' fma_loop_dev.cu\n",
    "git diff fma_loop_dev.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -Xcompiler '-fPIC'  -dc -o fma_loop_dev.o fma_loop_dev.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -Xcompiler '-fPIC' -dlink  fma_cuda.o fma_loop_dev.o -o fma_cuda_link.o\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "icpc -qopenmp -shared -Wl,-soname,libfma_cuda.so -o libfma_cuda.so fma_cuda_link.o fma_cuda.o fma_loop_dev.o -L/usr/local/pacerepov1/cuda/8.0.44/lib64 -Wl,-rpath,/usr/local/pacerepov1/cuda/8.0.44/lib64 -lcudart\n",
      "icpc -qopenmp -o fma_prof fma_prof.o fma_omp.o fma_loop_host.o libfma_cuda.so -Wl,-rpath,.\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1 nvprof --metrics flop_sp_efficiency ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "==23795== NVPROF is profiling process 23795, command: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==23795== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.\n",
      "==23795== Replaying kernel \"__nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==23795== Some kernel(s) will be replayed on device 1 in order to collect all events/metrics.\n",
      "==23795== Replaying kernel \"__nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==23795== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "==23795== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "[./fma_prof]: 4.436984e+00 elapsed seconds\n",
      "[./fma_prof]: 114688000000 flops executed\n",
      "[./fma_prof]: 2.584819e+10 flop/s\n",
      "==23795== Profiling application: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==23795== Profiling result:\n",
      "==23795== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      90.22%      90.22%      90.22%\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "Device \"Tesla P100-PCIE-16GB (1)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      89.71%      89.71%      89.71%\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000 PERF=\"nvprof --metrics flop_sp_efficiency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Okay, now we're seeing about ~90% efficiency (thought it took a lot of unrolling!)\n",
    "\n",
    "Now, according to the chart from last time:\n",
    "\n",
    "![NVIDIA comparison](../processors/images/nvidia-table.png)\n",
    "\n",
    "There are 64 single precision FPUs per SM on this Pascal GPU.  If I keep the same number of thread blocks, but set each block size to $64 / 2= 32$ (1 warp!), then there will be one thread per FPU.\n",
    "\n",
    "Because each thread's computation has a loop dependency (iteration $i + 1$ cannot start until iteration $i$ has completed), we should only be issuing one FMA at the rate that an operation can traverse the pipeline.  Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1 nvprof --metrics flop_sp_efficiency --metrics achieved_occupancy ./fma_prof 0 3584 32 112 8000000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 3584, T = 8000000, block size = 32\n",
      "==24563== NVPROF is profiling process 24563, command: ./fma_prof 0 3584 32 112 8000000 0.5 3.0\n",
      "==24563== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.\n",
      "==24563== Replaying kernel \"__nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==24563== Some kernel(s) will be replayed on device 1 in order to collect all events/metrics.\n",
      "==24563== Replaying kernel \"__nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==24563== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "==24563== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "[./fma_prof]: 1.874314e+01 elapsed seconds\n",
      "[./fma_prof]: 114688000000 flops executed\n",
      "[./fma_prof]: 6.118931e+09 flop/s\n",
      "==24563== Profiling application: ./fma_prof 0 3584 32 112 8000000 0.5 3.0\n",
      "==24563== Profiling result:\n",
      "==24563== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      15.73%      15.73%      15.73%\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.031250    0.031250    0.031250\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.456169    0.456169    0.456169\n",
      "Device \"Tesla P100-PCIE-16GB (1)\"\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      15.96%      15.96%      15.96%\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.031250    0.031250    0.031250\n",
      "    Kernel: __nv_static_56__43_tmpxft_000052c9_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.456129    0.456129    0.456129\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=32 Gs=$((56*2)) Nd=$((32*56*2)) T=8000000 PERF=\"nvprof --metrics flop_sp_efficiency --metrics achieved_occupancy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our achieved occupancy is ~1/32 = 32 / 1024, which makes sense.  Our efficiency is ~16%, so our estimated pipeline depth is **$1 / 0.16 \\approx 6$**.\n",
    "\n",
    "You could repeat this for the Kepler GPUs if you wanted, but the number would be quite different.\n",
    "As the chart above shows, the design of NVIDIA GPUs is not stable: Kepler has a few large SMs; Pascal has a lot of smaller SMs; Maxwell (not available on pace-ice) is in the middle, but almost completely abandons double precision arithmetic, which makes me sad :(."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're about to expand the types of programs we look at from those whose performance depends only on compute power to those whose performance also depends on the computers ability to move data to and from the processors.  We start with the simplest type of memory movement possible.\n",
    "\n",
    "## Streaming Kernels\n",
    "\n",
    "Sometimes our problems take the form of pure and simple *data parallelism*:\n",
    "\n",
    "- We have some kernel operation $f$ that has some mix of $\\color{blue}{inputs}$ and $\\color{red}{outputs}$ (some times a piece of data can be both).  Consider the example of\n",
    "  computing a particle's kinetic energy from it's momentum:\n",
    "  \n",
    "  $$\\Large \\color{red}{e} \\leftarrow f(\\color{blue}{m, u, v, w}) = \\frac{1}{2} m (u^2 + v^2 + w^2).$$\n",
    "  \n",
    "- If we want to do this more multiple particles, they can be computed independently:\n",
    "\n",
    "  $$\\large \\color{red}{e_i} \\leftarrow f(\\color{blue}{m_i, u_i, v_i, w_i}) = \\frac{1}{2} m_i (u_i^2 + v_i^2 + w_i^2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming we were processing each one from *registers*, how long would it take to compute the energy for $N$ particles?  Assuming $N$ is large, we would like that as a rate: **particle energies computed per second**.\n",
    "\n",
    "This is what last lecture was about:\n",
    "\n",
    "- **Simple upper bound (count the operations):** Divide the flop/s of whatever compute resource we have by the number of flops in the kernel.  Note the two terms in this ratio:\n",
    "\n",
    "  - flop/s in the *machine*\n",
    "  - flops in the *kernel* (algorithm)\n",
    "\n",
    "- **Pipeline based estimate (complicated):** Find the critical path; compute its length in pipeline cycles.  This gives the latency for computing one particle.  This should be a *lower bound*:\n",
    "  - If the whole kernel can be *vectorized*, then we can multiply by the vector width.\n",
    "  - If there are bubbles in the computation of one kernel that can be filled by the computation of another (ILP),\n",
    "    leading to a throughput that is higher than the inverse of latency\n",
    "    \n",
    "`TODO: whiteboard`\n",
    "    \n",
    "- **Measure (seems easy...):** Set up a bunch of particles and see how long it takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that I'm asking you to phrase your rate in machine independent units\n",
    "\n",
    "When people want to compare algorithms and machines for streaming kernels, the comparison that matters is the rate at which real things are accomplished.\n",
    "\n",
    "Let's say my kernel operation looks like lots of small sparse matrix-vector multiplications:\n",
    "\n",
    "- Algorithm A on machine 1 achieves 2 Tflop/s, but the way it computes its kernel is dense linear, and most of the operations that it performs are $c \\leftarrow c + 0*0$.\n",
    "\n",
    "- Algorithm B on machine 2 achives 2 Gflops/s, uses a sparse matrix format which requires more complicated instructions: it achieves a small fraction of Gflop/s, but each FMA is meaningful work $c \\leftarrow c + a *  b$.\n",
    "\n",
    "Which is better?  I haven't given you enough information to tell, because I've only given you machine dependent numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rules of thumb about measurement:\n",
    "\n",
    "#### - Usually we want to test something that is important, which means we do it over and over again\n",
    "\n",
    "Unless you know otherwise, assume that the thing you are trying to measure over and over again\n",
    "has *side effects* on the system that it is running on: that the system is adapting to your repetition.\n",
    "\n",
    "(Example we've already discussed: branch prediction)\n",
    "\n",
    "Because of that, it's usually good practice to **discard the first few timings**\n",
    "\n",
    "#### - Make your measurements reproducible\n",
    "\n",
    "Not only keep track of which version of the code you are measuring, but\n",
    "\n",
    "- how it was configured/compiled\n",
    "- what the environment variables were that affect it\n",
    "\n",
    "#### - Be aware of the precision of your timer relative to what you are trying to time\n",
    "\n",
    "If your timer calls a function (overhead of callstack operations, movement to and from memory, etc.) and you're measuring something that only lasts a few cycles, you will be measuring noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_loop_host.o fma_loop_host.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icpc -qopenmp -o fma_prof fma_prof.o fma_omp.o fma_loop_host.o libfma_cuda.so -Wl,-rpath,.\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1  ./fma_prof 10000000000 0 -1 -1 1 0.5 3.0\n",
      "[./fma_prof] Nh = 1410065408, Nd = 0, T = 1, default block size\n",
      "[./fma_prof]: 5.892210e-01 elapsed seconds\n",
      "[./fma_prof]: 2820130816 flops executed\n",
      "[./fma_prof]: 4.786202e+09 flop/s\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=10000000000 Nd=0 T=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fma_loop_host.o:     file format \u001b[33melf64-x86-64\u001b[39;49;00m\n",
      "\n",
      "\n",
      "Disassembly of section .text:\n",
      "\n",
      "\u001b[34m0000000000000000\u001b[39;49;00m <\u001b[32mfma_loop_host\u001b[39;49;00m>:\n",
      " * Input-Outputs:\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "   0:\t\u001b[34m48 89 d1 \u001b[39;49;00m            \t\u001b[32mmov\u001b[39;49;00m    \u001b[31m%rdx\u001b[39;49;00m,\u001b[31m%rcx\u001b[39;49;00m\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "   3:\t\u001b[34m85 ff \u001b[39;49;00m               \t\u001b[32mtest\u001b[39;49;00m   \u001b[31m%edi\u001b[39;49;00m,\u001b[31m%edi\u001b[39;49;00m\n",
      "   5:\t\u001b[34m0f 8e 05 01 00 00 \u001b[39;49;00m   \t\u001b[32mjle\u001b[39;49;00m    \u001b[34m110\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x110\u001b[39;49;00m>\n",
      "   b:\t\u001b[34m83 ff 10 \u001b[39;49;00m            \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m$0x10\u001b[39;49;00m,\u001b[31m%edi\u001b[39;49;00m\n",
      "   e:\t\u001b[34m0f 8c 04 01 00 00 \u001b[39;49;00m   \t\u001b[32mjl\u001b[39;49;00m     \u001b[34m118\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x118\u001b[39;49;00m>\n",
      "  14:\t\u001b[34m48 89 c8 \u001b[39;49;00m            \t\u001b[32mmov\u001b[39;49;00m    \u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      "  17:\t\u001b[34m48 83 e0 1f \u001b[39;49;00m         \t\u001b[32mand\u001b[39;49;00m    \u001b[31m$0x1f\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      "  1b:\t\u001b[34m89 c0 \u001b[39;49;00m               \t\u001b[32mmov\u001b[39;49;00m    \u001b[31m%eax\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      "  1d:\t\u001b[34m85 c0 \u001b[39;49;00m               \t\u001b[32mtest\u001b[39;49;00m   \u001b[31m%eax\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      "  1f:\t\u001b[34m74 10 \u001b[39;49;00m               \t\u001b[32mje\u001b[39;49;00m     \u001b[34m31\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x31\u001b[39;49;00m>\n",
      "  21:\t\u001b[34ma8 03 \u001b[39;49;00m               \t\u001b[32mtest\u001b[39;49;00m   \u001b[31m$0x3\u001b[39;49;00m,\u001b[31m%al\u001b[39;49;00m\n",
      "  23:\t\u001b[34m0f 85 ef 00 00 00 \u001b[39;49;00m   \t\u001b[32mjne\u001b[39;49;00m    \u001b[34m118\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x118\u001b[39;49;00m>\n",
      "  29:\t\u001b[34mf7 d8 \u001b[39;49;00m               \t\u001b[32mneg\u001b[39;49;00m    \u001b[31m%eax\u001b[39;49;00m\n",
      "  2b:\t\u001b[34m83 c0 20 \u001b[39;49;00m            \t\u001b[32madd\u001b[39;49;00m    \u001b[31m$0x20\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      "  2e:\t\u001b[34mc1 e8 02 \u001b[39;49;00m            \t\u001b[32mshr\u001b[39;49;00m    \u001b[31m$0x2\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      "  31:\t\u001b[34m8d 50 10 \u001b[39;49;00m            \t\u001b[32mlea\u001b[39;49;00m    \u001b[34m0x10\u001b[39;49;00m(\u001b[31m%rax\u001b[39;49;00m),\u001b[31m%edx\u001b[39;49;00m\n",
      "  34:\t\u001b[34m3b fa \u001b[39;49;00m               \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%edx\u001b[39;49;00m,\u001b[31m%edi\u001b[39;49;00m\n",
      "  36:\t\u001b[34m0f 8c dc 00 00 00 \u001b[39;49;00m   \t\u001b[32mjl\u001b[39;49;00m     \u001b[34m118\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x118\u001b[39;49;00m>\n",
      "  3c:\t\u001b[34m89 fa \u001b[39;49;00m               \t\u001b[32mmov\u001b[39;49;00m    \u001b[31m%edi\u001b[39;49;00m,\u001b[31m%edx\u001b[39;49;00m\n",
      "  3e:\t\u001b[34m33 f6 \u001b[39;49;00m               \t\u001b[32mxor\u001b[39;49;00m    \u001b[31m%esi\u001b[39;49;00m,\u001b[31m%esi\u001b[39;49;00m\n",
      "  40:\t\u001b[34m2b d0 \u001b[39;49;00m               \t\u001b[32msub\u001b[39;49;00m    \u001b[31m%eax\u001b[39;49;00m,\u001b[31m%edx\u001b[39;49;00m\n",
      "  42:\t\u001b[34m83 e2 0f \u001b[39;49;00m            \t\u001b[32mand\u001b[39;49;00m    \u001b[31m$0xf\u001b[39;49;00m,\u001b[31m%edx\u001b[39;49;00m\n",
      "  45:\t\u001b[34mf7 da \u001b[39;49;00m               \t\u001b[32mneg\u001b[39;49;00m    \u001b[31m%edx\u001b[39;49;00m\n",
      "  47:\t\u001b[34m03 d7 \u001b[39;49;00m               \t\u001b[32madd\u001b[39;49;00m    \u001b[31m%edi\u001b[39;49;00m,\u001b[31m%edx\u001b[39;49;00m\n",
      "  49:\t\u001b[34m48 85 c0 \u001b[39;49;00m            \t\u001b[32mtest\u001b[39;49;00m   \u001b[31m%rax\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      "  4c:\t\u001b[34m76 17 \u001b[39;49;00m               \t\u001b[32mjbe\u001b[39;49;00m    \u001b[34m65\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x65\u001b[39;49;00m>\n",
      "    a[i] = a[i] * b + c;\n",
      "  4e:\t\u001b[34mc5 fa 10 14 b1 \u001b[39;49;00m      \t\u001b[32mvmovss\u001b[39;49;00m (\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rsi\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m),\u001b[31m%xmm2\u001b[39;49;00m\n",
      "  53:\t\u001b[34mc4 e2 79 a9 d1 \u001b[39;49;00m      \t\u001b[32mvfmadd213ss\u001b[39;49;00m \u001b[31m%xmm1\u001b[39;49;00m,\u001b[31m%xmm0\u001b[39;49;00m,\u001b[31m%xmm2\u001b[39;49;00m\n",
      "  58:\t\u001b[34mc5 fa 11 14 b1 \u001b[39;49;00m      \t\u001b[32mvmovss\u001b[39;49;00m \u001b[31m%xmm2\u001b[39;49;00m,(\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rsi\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m)\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  5d:\t\u001b[34m48 ff c6 \u001b[39;49;00m            \t\u001b[32minc\u001b[39;49;00m    \u001b[31m%rsi\u001b[39;49;00m\n",
      "  60:\t\u001b[34m48 3b f0 \u001b[39;49;00m            \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%rax\u001b[39;49;00m,\u001b[31m%rsi\u001b[39;49;00m\n",
      "  63:\t\u001b[34m72 e9 \u001b[39;49;00m               \t\u001b[32mjb\u001b[39;49;00m     \u001b[34m4e\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x4e\u001b[39;49;00m>\n",
      " *\n",
      " * Input-Outputs:\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "  65:\t\u001b[34mc4 e2 7d 18 \u001b[39;49;00m         \u001b[33m\t(bad)  \u001b[39;49;00m\n",
      "  69:\t\u001b[34md9 c4 \u001b[39;49;00m               \t\u001b[32mfld\u001b[39;49;00m    \u001b[31m%st\u001b[39;49;00m(\u001b[34m4\u001b[39;49;00m)\n",
      "  6b:\t\u001b[34me2 7d \u001b[39;49;00m               \t\u001b[32mloop\u001b[39;49;00m   \u001b[34mea\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0xea\u001b[39;49;00m>\n",
      "  6d:\t\u001b[34m18 d0 \u001b[39;49;00m               \t\u001b[32msbb\u001b[39;49;00m    \u001b[31m%dl\u001b[39;49;00m,\u001b[31m%al\u001b[39;49;00m\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  6f:\t\u001b[34m48 63 f2 \u001b[39;49;00m            \t\u001b[32mmovslq\u001b[39;49;00m \u001b[31m%edx\u001b[39;49;00m,\u001b[31m%rsi\u001b[39;49;00m\n",
      "    a[i] = a[i] * b + c;\n",
      "  72:\t\u001b[34mc5 fc 10 24 81 \u001b[39;49;00m      \t\u001b[32mvmovups\u001b[39;49;00m (\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m),\u001b[31m%ymm4\u001b[39;49;00m\n",
      "  77:\t\u001b[34mc5 fc 10 6c 81 20 \u001b[39;49;00m   \t\u001b[32mvmovups\u001b[39;49;00m \u001b[34m0x20\u001b[39;49;00m(\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m),\u001b[31m%ymm5\u001b[39;49;00m\n",
      "  7d:\t\u001b[34mc4 e2 6d a8 e3 \u001b[39;49;00m      \t\u001b[32mvfmadd213ps\u001b[39;49;00m \u001b[31m%ymm3\u001b[39;49;00m,\u001b[31m%ymm2\u001b[39;49;00m,\u001b[31m%ymm4\u001b[39;49;00m\n",
      "  82:\t\u001b[34mc4 e2 6d a8 eb \u001b[39;49;00m      \t\u001b[32mvfmadd213ps\u001b[39;49;00m \u001b[31m%ymm3\u001b[39;49;00m,\u001b[31m%ymm2\u001b[39;49;00m,\u001b[31m%ymm5\u001b[39;49;00m\n",
      "  87:\t\u001b[34mc5 fc 11 24 81 \u001b[39;49;00m      \t\u001b[32mvmovups\u001b[39;49;00m \u001b[31m%ymm4\u001b[39;49;00m,(\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m)\n",
      "  8c:\t\u001b[34mc5 fc 11 6c 81 20 \u001b[39;49;00m   \t\u001b[32mvmovups\u001b[39;49;00m \u001b[31m%ymm5\u001b[39;49;00m,\u001b[34m0x20\u001b[39;49;00m(\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m)\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  92:\t\u001b[34m48 83 c0 10 \u001b[39;49;00m         \t\u001b[32madd\u001b[39;49;00m    \u001b[31m$0x10\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      "  96:\t\u001b[34m48 3b c6 \u001b[39;49;00m            \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%rsi\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      "  99:\t\u001b[34m72 d7 \u001b[39;49;00m               \t\u001b[32mjb\u001b[39;49;00m     \u001b[34m72\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x72\u001b[39;49;00m>\n",
      "  9b:\t\u001b[34m8d 42 01 \u001b[39;49;00m            \t\u001b[32mlea\u001b[39;49;00m    \u001b[34m0x1\u001b[39;49;00m(\u001b[31m%rdx\u001b[39;49;00m),\u001b[31m%eax\u001b[39;49;00m\n",
      "  9e:\t\u001b[34m3b f8 \u001b[39;49;00m               \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%eax\u001b[39;49;00m,\u001b[31m%edi\u001b[39;49;00m\n",
      "  a0:\t\u001b[34m72 6e \u001b[39;49;00m               \t\u001b[32mjb\u001b[39;49;00m     \u001b[34m110\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x110\u001b[39;49;00m>\n",
      "  a2:\t\u001b[34m48 63 ff \u001b[39;49;00m            \t\u001b[32mmovslq\u001b[39;49;00m \u001b[31m%edi\u001b[39;49;00m,\u001b[31m%rdi\u001b[39;49;00m\n",
      "  a5:\t\u001b[34m48 63 d2 \u001b[39;49;00m            \t\u001b[32mmovslq\u001b[39;49;00m \u001b[31m%edx\u001b[39;49;00m,\u001b[31m%rdx\u001b[39;49;00m\n",
      "  a8:\t\u001b[34m48 2b fa \u001b[39;49;00m            \t\u001b[32msub\u001b[39;49;00m    \u001b[31m%rdx\u001b[39;49;00m,\u001b[31m%rdi\u001b[39;49;00m\n",
      "  ab:\t\u001b[34m48 83 ff 04 \u001b[39;49;00m         \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m$0x4\u001b[39;49;00m,\u001b[31m%rdi\u001b[39;49;00m\n",
      "  af:\t\u001b[34m7c 63 \u001b[39;49;00m               \t\u001b[32mjl\u001b[39;49;00m     \u001b[34m114\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x114\u001b[39;49;00m>\n",
      "    a[i] = a[i] * b + c;\n",
      "  b1:\t\u001b[34m48 63 d2 \u001b[39;49;00m            \t\u001b[32mmovslq\u001b[39;49;00m \u001b[31m%edx\u001b[39;49;00m,\u001b[31m%rdx\u001b[39;49;00m\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  b4:\t\u001b[34m89 f8 \u001b[39;49;00m               \t\u001b[32mmov\u001b[39;49;00m    \u001b[31m%edi\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      "  b6:\t\u001b[34m83 e0 fc \u001b[39;49;00m            \t\u001b[32mand\u001b[39;49;00m    \u001b[31m$0xfffffffffffffffc\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      "  b9:\t\u001b[34m45 33 c0 \u001b[39;49;00m            \t\u001b[32mxor\u001b[39;49;00m    \u001b[31m%r8d\u001b[39;49;00m,\u001b[31m%r8d\u001b[39;49;00m\n",
      "  bc:\t\u001b[34m48 63 c0 \u001b[39;49;00m            \t\u001b[32mmovslq\u001b[39;49;00m \u001b[31m%eax\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      " *\n",
      " * Input-Outputs:\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "  bf:\t\u001b[34mc4 e2 79 18 \u001b[39;49;00m         \u001b[33m\t(bad)  \u001b[39;49;00m\n",
      "  c3:\t\u001b[34md9 c4 \u001b[39;49;00m               \t\u001b[32mfld\u001b[39;49;00m    \u001b[31m%st\u001b[39;49;00m(\u001b[34m4\u001b[39;49;00m)\n",
      "  c5:\t\u001b[34me2 79 \u001b[39;49;00m               \t\u001b[32mloop\u001b[39;49;00m   \u001b[34m140\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x140\u001b[39;49;00m>\n",
      "  c7:\t\u001b[34m18 d0 \u001b[39;49;00m               \t\u001b[32msbb\u001b[39;49;00m    \u001b[31m%dl\u001b[39;49;00m,\u001b[31m%al\u001b[39;49;00m\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "    a[i] = a[i] * b + c;\n",
      "  c9:\t\u001b[34m48 8d 34 91 \u001b[39;49;00m         \t\u001b[32mlea\u001b[39;49;00m    (\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rdx\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m),\u001b[31m%rsi\u001b[39;49;00m\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  cd:\t\u001b[34m49 83 c0 04 \u001b[39;49;00m         \t\u001b[32madd\u001b[39;49;00m    \u001b[31m$0x4\u001b[39;49;00m,\u001b[31m%r8\u001b[39;49;00m\n",
      "    a[i] = a[i] * b + c;\n",
      "  d1:\t\u001b[34mc5 f8 10 26 \u001b[39;49;00m         \t\u001b[32mvmovups\u001b[39;49;00m (\u001b[31m%rsi\u001b[39;49;00m),\u001b[31m%xmm4\u001b[39;49;00m\n",
      "  d5:\t\u001b[34mc4 e2 69 a8 e3 \u001b[39;49;00m      \t\u001b[32mvfmadd213ps\u001b[39;49;00m \u001b[31m%xmm3\u001b[39;49;00m,\u001b[31m%xmm2\u001b[39;49;00m,\u001b[31m%xmm4\u001b[39;49;00m\n",
      "  da:\t\u001b[34mc5 f8 11 26 \u001b[39;49;00m         \t\u001b[32mvmovups\u001b[39;49;00m \u001b[31m%xmm4\u001b[39;49;00m,(\u001b[31m%rsi\u001b[39;49;00m)\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  de:\t\u001b[34m48 83 c6 10 \u001b[39;49;00m         \t\u001b[32madd\u001b[39;49;00m    \u001b[31m$0x10\u001b[39;49;00m,\u001b[31m%rsi\u001b[39;49;00m\n",
      "  e2:\t\u001b[34m4c 3b c0 \u001b[39;49;00m            \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%rax\u001b[39;49;00m,\u001b[31m%r8\u001b[39;49;00m\n",
      "  e5:\t\u001b[34m72 e6 \u001b[39;49;00m               \t\u001b[32mjb\u001b[39;49;00m     \u001b[34mcd\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0xcd\u001b[39;49;00m>\n",
      "  e7:\t\u001b[34m48 3b c7 \u001b[39;49;00m            \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%rdi\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      "  ea:\t\u001b[34m73 24 \u001b[39;49;00m               \t\u001b[32mjae\u001b[39;49;00m    \u001b[34m110\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x110\u001b[39;49;00m>\n",
      "    a[i] = a[i] * b + c;\n",
      "  ec:\t\u001b[34m48 63 d2 \u001b[39;49;00m            \t\u001b[32mmovslq\u001b[39;49;00m \u001b[31m%edx\u001b[39;49;00m,\u001b[31m%rdx\u001b[39;49;00m\n",
      "  ef:\t\u001b[34m48 8d 0c 91 \u001b[39;49;00m         \t\u001b[32mlea\u001b[39;49;00m    (\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rdx\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m),\u001b[31m%rcx\u001b[39;49;00m\n",
      "  f3:\t\u001b[34m48 8d 14 81 \u001b[39;49;00m         \t\u001b[32mlea\u001b[39;49;00m    (\u001b[31m%rcx\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m,\u001b[34m4\u001b[39;49;00m),\u001b[31m%rdx\u001b[39;49;00m\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      "  f7:\t\u001b[34m48 ff c0 \u001b[39;49;00m            \t\u001b[32minc\u001b[39;49;00m    \u001b[31m%rax\u001b[39;49;00m\n",
      "    a[i] = a[i] * b + c;\n",
      "  fa:\t\u001b[34mc5 fa 10 12 \u001b[39;49;00m         \t\u001b[32mvmovss\u001b[39;49;00m (\u001b[31m%rdx\u001b[39;49;00m),\u001b[31m%xmm2\u001b[39;49;00m\n",
      "  fe:\t\u001b[34mc4 e2 79 a9 d1 \u001b[39;49;00m      \t\u001b[32mvfmadd213ss\u001b[39;49;00m \u001b[31m%xmm1\u001b[39;49;00m,\u001b[31m%xmm0\u001b[39;49;00m,\u001b[31m%xmm2\u001b[39;49;00m\n",
      " 103:\t\u001b[34mc5 fa 11 12 \u001b[39;49;00m         \t\u001b[32mvmovss\u001b[39;49;00m \u001b[31m%xmm2\u001b[39;49;00m,(\u001b[31m%rdx\u001b[39;49;00m)\n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      " 107:\t\u001b[34m48 83 c2 04 \u001b[39;49;00m         \t\u001b[32madd\u001b[39;49;00m    \u001b[31m$0x4\u001b[39;49;00m,\u001b[31m%rdx\u001b[39;49;00m\n",
      " 10b:\t\u001b[34m48 3b c7 \u001b[39;49;00m            \t\u001b[32mcmp\u001b[39;49;00m    \u001b[31m%rdi\u001b[39;49;00m,\u001b[31m%rax\u001b[39;49;00m\n",
      " 10e:\t\u001b[34m72 e7 \u001b[39;49;00m               \t\u001b[32mjb\u001b[39;49;00m     \u001b[34mf7\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0xf7\u001b[39;49;00m>\n",
      "    a[i] = a[i] * b + c;\n",
      "  }\n",
      "}\n",
      " 110:\t\u001b[34mc5 f8 77 \u001b[39;49;00m            \t\u001b[32mvzeroupper\u001b[39;49;00m \n",
      " 113:\t\u001b[34mc3 \u001b[39;49;00m                  \t\u001b[32mretq\u001b[39;49;00m   \n",
      " * a : the array\n",
      " \u001b[04m\u001b[31;01m*/\u001b[39;49;00m\n",
      "\u001b[36mvoid\u001b[39;49;00m\n",
      "fma_loop_host (\u001b[36mint\u001b[39;49;00m N, \u001b[36mint\u001b[39;49;00m T, \u001b[36mfloat\u001b[39;49;00m *a, \u001b[36mfloat\u001b[39;49;00m b, \u001b[36mfloat\u001b[39;49;00m c)\n",
      "{\n",
      "  \u001b[34mfor\u001b[39;49;00m (\u001b[36mint\u001b[39;49;00m i = \u001b[34m0\u001b[39;49;00m; i < N; i++) {\n",
      " 114:\t\u001b[34m33 c0 \u001b[39;49;00m               \t\u001b[32mxor\u001b[39;49;00m    \u001b[31m%eax\u001b[39;49;00m,\u001b[31m%eax\u001b[39;49;00m\n",
      " 116:\t\u001b[34meb cf \u001b[39;49;00m               \t\u001b[32mjmp\u001b[39;49;00m    \u001b[34me7\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0xe7\u001b[39;49;00m>\n",
      " 118:\t\u001b[34m33 d2 \u001b[39;49;00m               \t\u001b[32mxor\u001b[39;49;00m    \u001b[31m%edx\u001b[39;49;00m,\u001b[31m%edx\u001b[39;49;00m\n",
      " 11a:\t\u001b[34me9 7c ff ff ff \u001b[39;49;00m      \t\u001b[32mjmpq\u001b[39;49;00m   \u001b[34m9b\u001b[39;49;00m <\u001b[31mfma_loop_host\u001b[39;49;00m+\u001b[34m0x9b\u001b[39;49;00m>\n",
      " 11f:\t\u001b[34m90 \u001b[39;49;00m                  \t\u001b[32mnop\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "objdump -Sd fma_loop_host.o | pygmentize -l c-objdump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What did we see (hopefully):\n",
    "\n",
    "- `fma_loop_host`, with $T$ large out performs `fma_loop_host`, with $T=1$, even when we hard code and achieve the same vectorization, because with $T=1$ there is one load and store\n",
    "one **load** and one **store** for each FMA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load, Store, and Main Memory\n",
    "\n",
    "\n",
    "#### How long does a load take?  How long does a store take?\n",
    "\n",
    "This depends on whether the data is in *cache*.\n",
    "\n",
    "Today we are considering the rate at which we can apply a streaming kernel to a lot of data: so much that, even if we were using the data before, only a negligible percentage would already be in cache.\n",
    "\n",
    "**For the asymptotic rate of a streaming kernel, we care only about data that was originally in main memory, not in cache**\n",
    "\n",
    "Therefore, the numbers we need to know are:\n",
    "\n",
    "- The *latency* of moving data from memory to compute (secs)\n",
    "- The *streaming bandwidth* of moving data from memory to compute (bytes / sec)\n",
    "- The *bytes loaded* and *bytes stored* per kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A look at the hardware side\n",
    "\n",
    "[Hager and Wellein, Slides 68-82](https://moodle.rrze.uni-erlangen.de/pluginfile.php/12220/mod_resource/content/10/01_Arch.pdf)\n",
    "\n",
    "What did we learn?\n",
    "\n",
    "- Even if we only request a small amount of data in a load or write a small amount in a store, data moves to and from memory in increment of a *cache line*.  Achieving peak throughputs is only possible if you are accessing memory in sequence.\n",
    "\n",
    "  - One of our first examples in this class of how hardware rewards **locality**: using pieces of data that are close in space (location in memory) and time (one right after the other).\n",
    "  \n",
    "  - The same principles apply to GPUs, except that we have to think of the load of each thread as a vectorized load\n",
    "    of multiple pieces of data.  The loads from threads in a warp issue at the same time if they address \n",
    "    *consecutive locations* in memory.  (Called \"coalesced memory access\").  Uncoalesced memory access can get \n",
    "    serialized as multiple load instructions.\n",
    "    \n",
    "- The same principles as our previous lecture on processors alone apply: we don't see the latency of the memory transfer for large data sets because *hardware prefetching* is the pipeline parallelism of memory access.\n",
    "\n",
    "  - **One key difference:** If we want to model this as a pipeline, it is a heterogeneous pipeline.\n",
    "    - We cannot diagram them as boxes taking equal amount of time as before.\n",
    "    - The hardware units of throughput on the different components of the pipeline are different: one is\n",
    "      byte/s, the other is flop/s.  **How do we determine what the throughput rate of the full pipeline is?**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Benchmark for the hardware side: the STREAM benchmark\n",
    "\n",
    "[John D. McCalpin's STREAM slides](http://sites.utexas.edu/jdm4372/2016/11/22/sc16-invited-talk-memory-bandwidth-and-system-balance-in-hpc-systems/)\n",
    "\n",
    "`TODO if time: live demonstration`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "The throughput rate is going to depend on a ratio of ratios:\n",
    "\n",
    "- **Arithmetic Intensity:** (flops / kernel) / (bytes loaded and stored / kernel).  Units are flop / byte.\n",
    "\n",
    "- **Machine Balance:** (peak flop/s) / (peak byte/s).  Units are flop / byte.\n",
    "\n",
    "- These combine for the **roofline model**\n",
    "\n",
    "[Prof. Vuduc's GPU tuning slides, 12-26](http://vuduc.org/cse6230/slides/cse6230-fa14--07-gpu-tuning-1.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
